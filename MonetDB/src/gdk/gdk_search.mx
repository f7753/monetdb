@' The contents of this file are subject to the MonetDB Public
@' License Version 1.0 (the "License"); you may not use this file
@' except in compliance with the License. You may obtain a copy of
@' the License at
@' http://monetdb.cwi.nl/Legal/MonetDBLicense-1.0.html
@' 
@' Software distributed under the License is distributed on an "AS
@' IS" basis, WITHOUT WARRANTY OF ANY KIND, either express or
@' implied. See the License for the specific language governing
@' rights and limitations under the License.
@' 
@' The Original Code is the Monet Database System.
@' 
@' The Initial Developer of the Original Code is CWI.
@' Portions created by CWI are Copyright (C) 1997-2003 CWI.
@' All Rights Reserved.
@' 
@' Contributor(s):
@' 		Martin Kersten <Martin.Kersten@cwi.nl>
@' 		Peter Boncz <Peter.Boncz@cwi.nl>
@' 		Niels Nes <Niels.Nes@cwi.nl>
@' 		Stefan Manegold  <Stefan.Manegold@cwi.nl>

@f gdk_search
@a M. L. Kersten, P. Boncz

@* Search Accelerators

What sets BATs apart from normal arrays is their built-in ability to
search on both dimensions of the binary association.  The easiest way to 
implement this is simply walk to the whole table and compare against each 
element.  This method is of course highly inefficient, much better performance 
can be obtained if the BATs use some kind of index method to speed up
searching.

While index methods speed up searching they also have disadvantages.
In the first place extra storage is needed for the index. Second, 
insertion of data or removing old data requires updating of the index
structure, which takes extra time.

This means there is a need for both indexed and non-indexed BAT, the
first to be used when  little or no searching is needed, the
second to be used when searching is predominant. Also, there is no
best index method for all cases, different methods have different storage
needs and different performance. Thus, multiple index methods are provided, 
each suited to particular types of usage.

For query-dominant environments it pays to build a search accelerator.
The main problems to be solved are:
@T
\begin{itemize}
\item avoidance of excessive storage requirements, and
\item limited maintenance overhead.
\end{itemize}
@-
The BAT library automatically decides when an index becomes cost
effective.

In situations where an index is expected, a call is
made to @%BAThash@ or @%BATidx@.
Both operations check for indexing on the header.

@+ Interface Declarations
@h
#ifndef _GDK_SEARCH_H_
#define _GDK_SEARCH_H_
#include "gdk.h"

@- Hash indexing
This is a highly efficient implementation of simple bucket-chained hashing.

In the past, we used integer modulo for hashing, with bucket chains of mean size 4.
This was shown to be inferior to direct hashing with integer anding. The new implementation
reflects this.
@h
gdk_export size_t       BAThash      (BAT *b, int *collision);
gdk_export Heap*	HASHnew      (Hash *h, int tpe, size_t size, hash_t mask);
gdk_export hash_t	HASHmask     (size_t cnt);
gdk_export BAT*	        HASHprint    (BAT *b);
gdk_export void	        HASHremove   (BAT *b);
gdk_export void		HASHdestroy  (BAT *b);
gdk_export hash_t	HASHprobe    (Hash *h, ptr v); 
gdk_export hash_t	HASHlist     (Hash* h, size_t i) ;

#define mix_sht(X)            (((X)>>7)^(X))
#define mix_int(X)            (((X)>>7)^((X)>>13)^((X)>>21)^(X))
#define hash_loc(H,V)         hash_any(H,V)
#define hash_var(H,V)         hash_any(H,V)
#define hash_any(H,V)         (ATOMhash((H).type, (V)) & (H).mask) 
#define hash_chr(H,V)         ((hash_t) (*(unsigned char*) (V)) & (H).mask)
#define hash_sht(H,V)         ((hash_t) mix_sht(((unsigned short*) (V))[0]) & (H).mask)
#define hash_int(H,V)         ((hash_t) mix_int(((unsigned int*) (V))[0]) & (H).mask)
/* XXX return size_t-sized value for 8-byte oid? */
#define hash_lng(H,V)         ((hash_t) mix_int(((unsigned int*) (V))[0] ^ ((unsigned int*) (V))[1]) & (H).mask)


@= hashfnd
#define HASHfnd_@1(x,y,z)	{ hash_t _i; BUN _v; (x) = NULL;\
        if ((y)->hhash_heap || BAThash((y), NULL))\
        HASHloop_@1((y), (y)->hhash, _i, (z), _v) { (x) = _v; break; }\
}
@h
#define HASHfnd_str(x,y,z)	{ hash_t _i; (x) = NULL;\
        if ((y)->hhash_heap || BAThash((y), 0))\
        HASHloop_str((y), (y)->hhash, _i, (z)) { (x) = BUNptr((y),_i); break; }\
}
#define HASHfnd(x,y,z)	{ size_t _i; x=NULL;\
        if ((y)->hhash_heap || BAThash((y), 0))\
        HASHloop((y), (y)->hhash, _i, (z)) { (x) = BUNptr((y),_i); break; }\
}
@:hashfnd(chr)@
@:hashfnd(sht)@
@:hashfnd(int)@
@:hashfnd(lng)@

#if SIZEOF_VOID_P == SIZEOF_INT
#define HASHfnd_ptr(x,y,z)	HASHfnd_int(x,y,z)
#else /* SIZEOF_VOID_P == SIZEOF_LONG */
#define HASHfnd_ptr(x,y,z)	HASHfnd_lng(x,y,z)
#endif
#define HASHfnd_bit(x,y,z)	HASHfnd_chr(x,y,z)
#if SIZEOF_OID == SIZEOF_INT	/* OIDDEPEND */
#define HASHfnd_oid(x,y,z)	HASHfnd_int(x,y,z)
#else
#define HASHfnd_oid(x,y,z)	HASHfnd_lng(x,y,z)
#endif
#define HASHfnd_flt(x,y,z)	HASHfnd_int(x,y,z)
#define HASHfnd_dbl(x,y,z)	HASHfnd_lng(x,y,z)
#define HASHfnd_any(x,y,z)	HASHfnd(x,y,z)
@- 
A new entry is added with @%HASHins@ using the BAT, the BUN index, 
and a pointer to the value to be stored. An entry is removed by @%HASdel@.

@= hashins
#define HASHins_@1(h, i, v) {\
	hash_t _c = hash_@1(h,v);\
	h.link[i] = h.hash[_c];\
	h.hash[_c] = i; }
@h
#define HASHins(h,i,v) {\
	hash_t _c = HASHprobe(&h, v);\
	h.link[i] = h.hash[_c];\
	h.hash[_c] = i; }

#if SIZEOF_VOID_P == SIZEOF_INT
#define HASHins_ptr(h,i,v)	HASHins_int(h,i,v)
#else /* SIZEOF_VOID_P == SIZEOF_LONG */
#define HASHins_ptr(h,i,v)	HASHins_lng(h,i,v)
#endif
#define HASHins_bit(h,i,v)	HASHins_chr(h,i,v)
#if SIZEOF_OID == SIZEOF_INT	/* OIDDEPEND */
#define HASHins_oid(h,i,v)	HASHins_int(h,i,v)
#else
#define HASHins_oid(h,i,v)	HASHins_lng(h,i,v)
#endif
#define HASHins_flt(h,i,v)	HASHins_int(h,i,v)
#define HASHins_dbl(h,i,v)	HASHins_lng(h,i,v)
#define HASHins_str(h,i,v)	HASHins(h,i,v)
#define HASHins_any(h,i,v)	HASHins(h,i,v)
#define HASHinsvar(h,i,v)	HASHins(h,i,v)
#define HASHinsloc(h,i,v)	HASHins(h,i,v)

@:hashins(chr)@
@:hashins(sht)@
@:hashins(int)@
@:hashins(lng)@

#define HASHdel(h, i, v, next) {\
        if (next && h.link[i+1] == i) {\
                h.link[i+1] = h.link[i];\
        } else {\
		size_t _c = HASHprobe(&h, v);\
		if (h.hash[_c] == i) {\
			h.hash[_c] = h.link[i]; \
		} else\
		for(_c = h.hash[_c]; _c; _c = h.link[_c]) {\
			if (h.link[_c] == i) {\
				h.link[_c] = h.link[i]; \
				break;\
			}\
		}\
	} h.link[i] = 0;\
}

#define HASHmove(h, i, j, v, next) { \
        if (next && h.link[i+1] == i) {\
                h.link[i+1] = j;\
        } else {\
		size_t _c = HASHprobe(&h, v); \
		if (h.hash[_c] == i) {\
			h.hash[_c] = j; \
		} else\
		for(_c = h.hash[_c]; _c; _c = h.link[_c]) {\
			if (h.link[_c] == i) {\
				h.link[_c] = j; \
				break;\
			}\
		}\
	} h.link[j] = h.link[i];\
}

@- Hash Table Creation
@T
The @`hash@9 indexing scheme for BATs reserves a block of memory to maintain
the hash table and a collision list. A one-to-one mapping exists between
the BAT and the collision list using the BUN index. NOTE: we alloc the
link list as a parallel array to the BUN array; hence the hash link array has
the same size as BATcapacity(b) (not BATcount(b)). This allows us in the
BUN insert and delete to assume that there is hash space iff there is BUN
space. If there is no BUN space, the BATextend now destroys the hash table.

The hash mask size is a power of two, so we can do bitwise AND on the 
hash (integer) number to quickly find the head of the bucket chain.  
Clearly, the hash mask size is a crucial parameter. If we know that the column
is unique (hkey), we use direct hashing (mask size ~= BATcount). Otherwise
we dynamically determine the mask size by starting out with mask size = BATcount/64 
(just 1.5% of memory storage overhead). Then we start building the hash table 
on the first 25% of the BAT. As we aim for max-collisions list length of 4,
the list on 25% should not exceed length 1. So, if a small number of collisssions
occurs (mask/2) then we abandon the attemp and restart with a mask that is 4 times 
larger. This converges after three cycles to direct hashing.
@c
#include "gdk.h"

hash_t HASHmask(size_t cnt) {
	size_t m = 8; /* minumum size */
	while(m+m < cnt) m += m;
	if (m+m - cnt < cnt - m) m += m;
  	return m;
}

void HASHclear(Hash *h) {
	hash_t *i, *j;
	for (i = h->hash, j = i + h->mask; i <= j; i++) {
		*i = 0;
	}
}

Heap* HASHnew(Hash *h, int tpe, size_t size, hash_t mask) { 
	Heap *hp = (Heap*) GDKmalloc(sizeof(Heap));
	HEAPalloc(hp, mask+ size + 1, sizeof(hash_t)); /* +1 is for the hole */
        h->lim = size;
	h->mask = mask - 1;
        h->link = (hash_t*) hp->base;
        h->hash = h->link + h->lim;
        h->type = tpe;
	HASHclear(h); /* zero the mask */
	return hp;
}

@= starthash
	for(xx=BUNsize(b); r < p; r += xx) { 
		ptr v = BUN@2(b, r); 
		hash_t c = hash_@1((*h), v);
		if (h->hash[c] == 0 && nslots-- == 0) {
			break; /* mask too full */
		}
		h->link[i] = h->hash[c];
		h->hash[c] = (hash_t) i++;
	} break; 
@= finishhash
	for(xx=BUNsize(b); p < q; p += xx) { 
		ptr v = BUN@2(b, p); 
		hash_t c = hash_@1((*h), v);
		h->link[i] = h->hash[c]; 
		h->hash[c] = (hash_t) i++;
	} break; 
@- 
The prime routine for the BAT layer is to create a new hash index.
Its argument is the element type and the maximum number of BUNs
be stored under the hash function. 
@c
size_t BAThash(BAT *b, int *collision) {
    MT_set_lock(GDKhashLock[ABS(b->batCacheid)&BBPLOCKMASK], "BAThash");
    if (b->hhash_heap == NULL) {
	unsigned int xx, tpe = ATOMstorage(b->htype);
	size_t i, cnt = BATcount(b);
	hash_t mask;
	BUN p = BUNfirst(b), q = BUNlast(b), r;
	Hash* h = &b->hhash;
	Heap *hp = NULL;

	/* cnt = 0, hopefully there is a proper capacity from which
	 * we can derive enough information */
	if (!cnt) cnt = BATcapacity(b);

	if (b->htype == TYPE_void) {
		BATDEBUG GDKwarning("BAThash: creating hash-table on void column..\n");
		tpe = TYPE_void;
	}
	/* determine hash mask size */
	if (collision) {
		mask = HASHmask(cnt / *collision); /* p = first; so no dynamic scheme */
	} else if (b->hkey) {
		mask = HASHmask(cnt); /* p = first; so no dynamic scheme */
	} else {
		/* dynamic hash: we start with HASHmask(cnt/64); if there are too many collisions 
		 * we try HASHmask(cnt/16), then HASHmask(cnt/4), and finally HASHmask(cnt).  */ 
		mask = HASHmask(cnt >> 6);
		p += (cnt>>2) * BUNsize(b); /* try out on first 25% of b */
		if (p>q) p = q;
	}
	do {
		size_t nslots = mask >> 1; /* 1/2 full is too full */
		r = BUNfirst(b);
		i = BUNindex(b, r); 
		if (hp) {
			HEAPfree(hp);
			GDKfree(hp);
		}
		/* create the hash structures */
		hp = HASHnew(h, ATOMtype(b->htype), BATcapacity(b), (size_t)mask);

		switch(tpe) {
		case TYPE_chr: @:starthash(chr,hloc)@
		case TYPE_sht: @:starthash(sht,hloc)@
		case TYPE_int:
		case TYPE_flt: @:starthash(int,hloc)@
		case TYPE_dbl:
		case TYPE_lng: @:starthash(lng,hloc)@
		default:       @:starthash(any,head)@
		}
	} while(r < p && mask < cnt && (mask <<= 2));

	/* finish the hashtable with the current mask */
	p = r;
	switch(tpe) {
	case TYPE_chr: @:finishhash(chr,hloc)@
	case TYPE_sht: @:finishhash(sht,hloc)@
	case TYPE_int:
	case TYPE_flt: @:finishhash(int,hloc)@
	case TYPE_dbl:
	case TYPE_lng: @:finishhash(lng,hloc)@
	default:       @:finishhash(any,head)@
	}
        b->hhash_heap = BATmirror(b)->thash_heap = hp; 
    }
    MT_unset_lock(GDKhashLock[ABS(b->batCacheid)&BBPLOCKMASK], "BAThash");
    return 1; 
}

@- 
The entry on which a value hashes can be calculated with the 
routine @%HASHprobe@. 
@c
hash_t HASHprobe(Hash *h, ptr v) {
	switch(ATOMstorage(h->type)) {
	case TYPE_chr: return hash_chr(*h,v);
	case TYPE_sht: return hash_sht(*h,v);
	case TYPE_int:
	case TYPE_flt: return hash_int(*h,v);
	case TYPE_dbl:
	case TYPE_lng: return hash_lng(*h,v);
	}
	return hash_any(*h,v);
}

BAT *HASHprint(BAT *b) {
#if SIZEOF_OID == SIZEOF_INT
	BAT *bn = BATnew(BAThtype(b), TYPE_int, BATcount(b));
#else
	BAT *bn = BATnew(BAThtype(b), TYPE_lng, BATcount(b));
#endif
	BUN p,q;

	if (!(b && b->hhash_heap)) return 0;
	BATloop(b,p,q) {
		size_t i = HASHprobe(&b->hhash, BUNhead(b,p));
		BUNfastins(bn, BUNhead(b,p), &i); 
	}
	bn->hsorted = BAThordered(b);
	bn->tsorted = FALSE;
	return bn;
}

hash_t HASHlist(Hash* h, size_t i) {
        hash_t j;
        hash_t c = 1;

        while ((j = h->link[i]) > 0) {
                c++;
                i = j;
                if (i > h->lim) {
                        stream_printf(GDKout, "hash inconsistency link %d\n",i);
                        break;
                }
        }
        return c;
}

void HASHremove(BAT *b) {
	if (b->hhash_heap) {
		BAT *p = BBP_cache(VIEWparent(b));
		if (p && b->hhash_heap == p->hhash_heap) {
			GDKerror("HASHremove: view-bat cannot remove parent hash-table\n");
		} else {
			HEAPfree(b->hhash_heap);
			GDKfree(b->hhash_heap);
		}
		memset(&b->hhash, 0, sizeof(Hash));
		BATmirror(b)->thash_heap = b->hhash_heap = NULL;
	}
}

void HASHdestroy(BAT *b) {
	HASHremove(b);
	HASHremove(BATmirror(b));
}




@+  Binary Index Trees
In this @`index@9 implementation we have choosen for attachment of a sorted list
of BUNs to a column whenever a primitive warrants its construction to
speed-up (future) processing. This sorted list is discarded as soon as it 
becomes invalid by an update on the underlying structure.
This approach favours query dominant environments.

The index is an auxilary structure. The sort order can be reflected
in the BUN ordering by applying @%BATsort@, which produces a copy
where the elements are put in the proper sort order.

The sorted list is build using a pointer-based binary search tree, which
is discarded when the sorting order has been determined. Experience
has shown that this is more effective than calling @%qsort()@.

[The implementation based on Index seems to be more expensive
when no code expansion is being used. It sorts 10K in about 10 seconds.
I expect the BST to be much faster at an expense of 100-200% space overhead.
This is true. @%qsort@ takes about 570 ms for unique1.
Code compression further reduced it to ca. 300 ms.  ]

PETER: the whole index tree structure is questionable. I suspect that
sorting with the monet-internal qsort is much much quicker. Therefore
there is no justification anymore for index trees. Let's make Monet
less complicated and kill them all/go for qsort and direct binary search 
instead for accelerating range predicates. (TODO)

@- Binary Index Trees
@h
/* type specific binary search implementations */
gdk_export BUN	IDXfnd_chr       (BAT *b, ptr v);
gdk_export BUN	IDXfnd_sht       (BAT *b, ptr v);
gdk_export BUN	IDXfnd_int       (BAT *b, ptr v);
gdk_export BUN	IDXfnd_flt       (BAT *b, ptr v);
gdk_export BUN	IDXfnd_lng       (BAT *b, ptr v);
gdk_export BUN	IDXfnd_dbl       (BAT *b, ptr v);
gdk_export BUN	IDXfnd_loc       (BAT *b, ptr v);
gdk_export BUN	IDXfnd_var       (BAT *b, ptr v);
gdk_export BUN*	IDXfndfirst_chr  (BAT *b, ptr v);
gdk_export BUN*	IDXfndfirst_sht  (BAT *b, ptr v);
gdk_export BUN*	IDXfndfirst_int  (BAT *b, ptr v);
gdk_export BUN*	IDXfndfirst_flt  (BAT *b, ptr v);
gdk_export BUN*	IDXfndfirst_lng  (BAT *b, ptr v);
gdk_export BUN*	IDXfndfirst_dbl  (BAT *b, ptr v);
gdk_export BUN*	IDXfndfirst_loc  (BAT *b, ptr v);
gdk_export BUN*	IDXfndfirst_var  (BAT *b, ptr v);
gdk_export BUN*	IDXfndlast_chr   (BAT *b, ptr v);
gdk_export BUN*	IDXfndlast_sht   (BAT *b, ptr v);
gdk_export BUN*	IDXfndlast_int   (BAT *b, ptr v);
gdk_export BUN*	IDXfndlast_flt   (BAT *b, ptr v);
gdk_export BUN*	IDXfndlast_lng   (BAT *b, ptr v);
gdk_export BUN*	IDXfndlast_dbl   (BAT *b, ptr v);
gdk_export BUN*	IDXfndlast_loc   (BAT *b, ptr v);
gdk_export BUN*	IDXfndlast_var   (BAT *b, ptr v);

@- 
Once a BAT is being destroyed, it should also take care of the indices
maintained.
@c
void IDXremove(BAT *b) {
	if (b->hidx_heap) {
		BAT *p = BBP_cache(VIEWparent(b));
		if (p && b->hidx_heap == p->hidx_heap) {
			GDKerror("IDXremove: view-bat cannot remove parent idx-tree\n");
		} else {
			HEAPfree(b->hidx_heap);
		}
		memset(&b->hidx, 0, sizeof(IDX));
		BATmirror(b)->tidx_heap = b->hidx_heap = NULL; 
	}
}


void IDXdestroy(BAT *b) {
	IDXremove(b);
	IDXremove(BATmirror(b));
}
@- 
Insertion is straight forward, 
The intermediate tree should be built around the hloc directly. This
can speed-up processing significantly.

@= idxins
{	Tnode *t = b->hidx.root; 
	Tnode *f = b->hidx.free; 
	
	b->hidx.free++; 
	f->v = @4;

	for (;;) {
		if (t == NULL) {	
       			b->hidx.root = f; 
			break;
		}
		if (@1_LT(BUNh@3(b,@4), BUNh@3(b,t->v), @2)) {
			if (t->l == NULL) { 
				t->l = f;
		       		break;
			}
			t = t->l; 
		} else {
			if (t->r == NULL) {
				t->r = f;
				break;
			}
			t = t->r; 
		}
       	}
} 
@- 
First the pointer-based tree is constructed, thereafter it is
emptied in the pointer list. This saves significant space.

The routine randomizes processing of its input to avoid linear
binary search trees later on.

@= idxinsloop
	while (i > 0) {
		r += rand();
		j = r % i; 
		@:idxins(@1,@2,@3,lnk[j])@
		lnk[j] = lnk[--i]; 
	} break;
@= idxhistoloop
	BATloopFast(b, p, q, i) {
		BUN r = SORTfnd_@1(histo, BUNh@2(b,p));
		if (!r) { cur = ~(size_t)0 /*(size_t)(-1)*/; break; }
		j = BUNindex(histo, r) - off;
		*base[j] = p; base[j]++;
	} break;
@c
void IDXcollect(Index vt, Tnode	*t){
	if (t == NULL) {
		return; 
	}
	IDXcollect(vt, t->l); 
	*vt->last = t->v; 
	vt->last++; 
	IDXcollect(vt, t->r); 
}

int BATidx(BAT *b, BAT *h) {
    if (b->htype == TYPE_void || !ATOMlinear(b->htype)) {
	    return 0;
    }
    MT_set_lock(GDKidxLock[ABS(b->batCacheid)&BBPLOCKMASK], "BATidx");
    if (b->hidx_heap == NULL) {
	    size_t cnt = BATcount(b);
	    BAT *unique = h;
	    size_t r = 0;
	    BUN p, q, *lnk; 
	    size_t i, j;
	    Index idx = &b->hidx;
	    Heap tmp, *hp = (Heap*) GDKmalloc(sizeof(Heap));

	    HEAPalloc(hp, MAX(1,cnt), sizeof(BUN));
	    lnk = idx->base = (BUN*) hp->base; 
	    idx->last = idx->first = idx->base; 
	    idx->root = 0; 
@-
dynamic check using sampling to see whether there are many doubles.
@c
#define IDX_MANY_DUPLICATES	4
	    if (b->hkey == 0 && cnt > 10000 && unique == NULL) {
		BAT *s = BATsample(b, MIN(BATcount(b),1000));
		BAT *v = BAThistogram(BATmirror(s));
		if (BATcount(v) * IDX_MANY_DUPLICATES < BATcount(b) ) {
		    unique = BAThistogram(BATmirror(b));
		} 
		BBPreclaim(s);
		BBPreclaim(v);
	    } 
@- normal case
Use randomized order for insert elements to get a nicely balanced tree
@c
	    if (unique == NULL) {
		HEAPalloc(&tmp, MAX(1,cnt), sizeof(Tnode));
		idx->stack = (Tnode*) tmp.base;
		idx->free = idx->stack; 
		memset((str)idx->stack, 0, cnt*sizeof(Tnode)); 
		BATloopFast(b, p, q, i) {
			*(lnk++) = p; 
		}
		i = lnk - idx->base; 
		lnk = idx->base; 

		switch(ATOMstorage(b->htype)) {
		case TYPE_chr:	@:idxinsloop(simple,chr,loc)@
		case TYPE_sht:	@:idxinsloop(simple,sht,loc)@
		case TYPE_int:	@:idxinsloop(simple,int,loc)@
		case TYPE_flt:	@:idxinsloop(simple,flt,loc)@
		case TYPE_dbl:	@:idxinsloop(simple,dbl,loc)@
		case TYPE_lng:	@:idxinsloop(simple,lng,loc)@
		default:
			if (b->hvarsized) {
				@:idxinsloop(atom,b->htype,var)@
			} else {
				@:idxinsloop(atom,b->htype,loc)@
			}
		}
		IDXcollect(&b->hidx, b->hidx.root); 

@- many duplicates
construction in one pass from a sorted histogram 
@c
	    } else {
		size_t off, cur = 0;
		BUN **base, **map;
		BAT *histo;

 		if (BAThordered(unique)&1) {
			histo = unique;
		} else {
		   	histo = BATsort(unique);
	        	if (unique != h) BBPunfix(unique->batCacheid);
		}
 		off = BUNindex(histo, BUNfirst(histo));
		HEAPalloc(&tmp, BATcount(histo), sizeof(BUN*));
 		base = map = (BUN**) tmp.base;

		BATloopFast(histo, p, q, i) {
			*(map++) = lnk + cur;
			cur += *(int*) BUNtloc(histo,p);
		}
		if (cur == cnt) 
		switch(ATOMstorage(b->htype)) {
		case TYPE_chr:	@:idxhistoloop(chr,loc)@
		case TYPE_sht:	@:idxhistoloop(sht,loc)@
		case TYPE_int:	@:idxhistoloop(int,loc)@
		case TYPE_flt:	@:idxhistoloop(flt,loc)@
		case TYPE_dbl:	@:idxhistoloop(dbl,loc)@
		case TYPE_lng:	@:idxhistoloop(lng,loc)@
		default:
			if (b->hvarsized) {
				@:idxhistoloop(var,var)@
			} else {
				@:idxhistoloop(loc,loc)@
			}
		}
		if (histo != unique || h == NULL) {
			BBPunfix(histo->batCacheid);
		}
		idx->last = idx->first + cnt;
	    }
	    HEAPfree(&tmp);
	    idx->stack = idx->root = 0; 
	    b->hidx_heap = BATmirror(b)->tidx_heap = hp; 
    }
    MT_unset_lock(GDKidxLock[ABS(b->batCacheid)&BBPLOCKMASK], "BATidx");
    return 1; 
}


@- binary search
Binary searching is used to locate elements in the sorted list. 
The routine @%IDXfnd@ locates the first element in the list equal to
the value. A NULL is returned when the value cannot be found.

Type-optimized versions are available as well. 
This macro expands to @`IDXfnd_chr@5,  @`IDXfnd_sht@5, @`IDXfnd_int@5,
@`IDXfnd_flt@5, @`IDXfnd_lng@5, @`IDXfnd_dbl@5, @`IDXfnd_loc@5 and
@`IDXfnd_var@5.

@= idxfnd
INLINE BUN IDXfnd_@2(BAT *b, ptr v) {
	int	i, var, loc; 
	BUN	*low;
	BUN	*up;
	BUN	*middle; 

	var = loc = b->htype;
	(void)var;
	(void)loc;
	low = b->hidx.first; 
	up = b->hidx.last-1; 
	do {
		middle = up - ((up - low) / 2); 
		i = @1_CMP(v, BUNh@3(b,(*middle)),@2);
		if (i == 0) {
			return *middle; 
		}
		if (i < 0) {
			up = middle-1; 
		} else { 
		       	low = middle + 1; 
		}
	} while (low <= up); 

	if (i > 0) {
		middle--; 
	}
       	i = @1_EQ(v, BUNh@3(b,(*middle)),@2);
	return i?*middle:0; 
}
@c
@:idxfnd(simple,chr,loc)@
@:idxfnd(simple,sht,loc)@
@:idxfnd(simple,int,loc)@
@:idxfnd(simple,flt,loc)@
@:idxfnd(simple,lng,loc)@
@:idxfnd(simple,dbl,loc)@
@:idxfnd(atom,var,var)@
@:idxfnd(atom,loc,loc)@

BUN IDXfnd(BAT *b, ptr v){
	FATALcheck(b->hidx_heap == NULL, "IDXfnd: tree expected"); 
	FATALcheck(v == NULL, "IDXfnd: value expected"); 

	switch(ATOMstorage(b->htype)) {
	case TYPE_chr:	return IDXfnd_chr(b,v);
	case TYPE_sht:	return IDXfnd_sht(b,v);
	case TYPE_int:	return IDXfnd_int(b,v);
	case TYPE_flt:	return IDXfnd_flt(b,v);
	case TYPE_dbl:	return IDXfnd_dbl(b,v);
	case TYPE_lng:	return IDXfnd_lng(b,v);
	}
	if (b->hvarsized) {
		return IDXfnd_var(b,v);
	}
	return IDXfnd_loc(b,v);
}

@- 
To optimize searching, we need a method that calculates the 
qualifying range within a sorted list. 

@= idxrng
	low = b->hidx.first; 
	up = b->hidx.last-1; 
	do {
		middle = up - ((up - low)/2);
		i = @1_CMP(v, BUNh@3(b,(*middle)),@2);
		if (i == 0) {
			break; 
		}
		if (i < 0) {
			middle--;
			up = middle;
		} else {
			middle++;
			low = middle; 
		}
	} while (low <= up);
@= idxlasteq
	if (i == 0) {
		while (++middle < b->hidx.last)
			if (!@1_EQ(v, BUNh@3(b, (*middle)), @2)) break; 
	} else if (i < 0) {
		 middle++;
	}
	return middle; 
@= idxfirsteq
	if (i == 0) {
		while (--middle >= b->hidx.first)
			if (!@1_EQ(v, BUNh@3(b, (*middle)), @2)) break; 
	} else if (i > 0) {
		 middle--;
	}
	return middle+1; 
@-
The @`IDXfndlast@5 operation finds the last BUN equal to a certain value 
in a column with a binary index (or NULL if not found). Type specific 
versions are available through macro expansions 
@`IDXfndlast_chr@5, @`IDXfndlast_sht@5, @`IDXfndlast_int@5, @`IDXfndlast_flt@5,
@`IDXfndlast_lng@5, @`IDXfndlast_dbl@5, @`IDXfndlast_loc@5 and
@`IDXfndlast_var@5.

The @`IDXfndfirst@5 operation finds the last BUN equal to a certain value 
in a column with a binary index (or NULL if not found). Type specific 
versions are available through macro expansions 
@`IDXfndfirst_chr@5, @`IDXfndfirst_sht@5, @`IDXfndfirst_int@5, 
@`IDXfndfirst_flt@5, @`IDXfndfirst_lng@5, @`IDXfndfirst_dbl@5, 
@`IDXfndfirst_loc@5 and @`IDXfndfirst_var@5.

@= idxfndfcn
INLINE BUN *IDXfnd@4_@5(BAT *b, ptr v){
	int	i;
	BUN	*low;
	BUN	*up;
	BUN	*middle; 

	@:idxrng(@1,@2,@3)@
	@:idx@4eq(@1,@2,@3)@
}
@= idxfndwhich
@:idxfndfcn(simple,chr,loc,@1,chr)@
@:idxfndfcn(simple,sht,loc,@1,sht)@
@:idxfndfcn(simple,int,loc,@1,int)@
@:idxfndfcn(simple,flt,loc,@1,flt)@
@:idxfndfcn(simple,lng,loc,@1,lng)@
@:idxfndfcn(simple,dbl,loc,@1,dbl)@
@:idxfndfcn(atom,b->htype,var,@1,var)@
@:idxfndfcn(atom,b->htype,loc,@1,loc)@

BUN *IDXfnd@1(BAT *b, ptr v){
	FATALcheck(b->hidx_heap == NULL, "IDXfnd@1: tree expected"); 
	FATALcheck(v == NULL, "IDXfnd@1: value expected"); 

	if (BATcount(b) <= 0) return b->H->idx.first;

	switch(ATOMstorage(b->htype)) {
	case TYPE_chr:	return IDXfnd@1_chr(b,v);
	case TYPE_sht:	return IDXfnd@1_sht(b,v);
	case TYPE_int:	return IDXfnd@1_int(b,v);
	case TYPE_flt:	return IDXfnd@1_flt(b,v);
	case TYPE_dbl:	return IDXfnd@1_dbl(b,v);
	case TYPE_lng:	return IDXfnd@1_lng(b,v);
	}
	if (b->hvarsized) {
		return IDXfnd@1_var(b,v);
	}
	return IDXfnd@1_loc(b,v);
}
@c
@:idxfndwhich(first)@
@:idxfndwhich(last)@

void IDXrng_(BAT *b, ptr v, ptr w, bit li, bit hi, BUN **p, BUN **q){
	int (*cmp)(ptr,ptr) = BATatoms[b->htype].atomCmp;
	int res = (*cmp)(v, w);
	if (res > 0) {
		{ptr _swap = v; v = w; w = _swap;}
		{bit _si = li; li = hi; hi = _si;}
	}
	if (li) *p = IDXfndfirst(b,v);
	else    *p = IDXfndlast(b,v);
	if (res) {
		if (hi) *q = IDXfndlast(b,w);
		else    *q = IDXfndfirst(b,w);
	} else if (!(li && hi)) {
		*q = *p;
	} else {
		BUN *cur, *end = IDXlast(b);
		for(cur = *p; cur < end && (cmp)(BUNhead(b,*cur),v) == 0; cur++);
		*q = cur;
	}
}

void IDXrng(BAT *b, ptr v, ptr w, BUN **p, BUN **q){
	IDXrng_(b, v, w, TRUE, TRUE, p, q);
}

@+ Binary Search on a Sorted BAT
We have two main routines, @`SORTfndfirst@5(b,v) and @`SORTfndlast@5(b,v), that 
search for a TAIL value 'v' in a sorted BAT. If the value is present, it 
returns a pointer to its first occurence; else it returns @%BUNfirst(b)@.
The latter routine returns a pointer to the BUN just after the last 
occurence of 'v', else it returns @%BUNlast(b)@.
@
@
From the above routines we now also defined the @`SORTfnd@5 and @%SORTfnd_tpe@
routines that look vor a certain value and return a (not necesarrily the first
or last) reference to it, or NULL (if the value does not exist).

Note: of the SORTfnd and IDXfnd macros, only @`SORTfndfirst@5(b,v) and 
@`SORTfndlast@5(b,v) work on the tail of a bat!
@
@

@- Range Binary Search
Type-specific versions of @:SORTfndfirst@ are available using macro 
expansions. It finds the first BUN larger or equal than a certain
value and expands to @`SORTfndfirst_chr@5, @`SORTfndfirst_sht@5,
@`SORTfndfirst_int@5, @`SORTfndfirst_flt@5, @`SORTfndfirst_lng@5,
@`SORTfndfirst_dbl@5, @`SORTfndfirst_loc@5 and @`SORTfndfirst_var@5.

Type-specific versions of @:SORTfndlast@ are available using macro 
expansions. It finds the first BUN smaller or equal than a certain
value and expands to @`SORTfndlast_chr@5, @`SORTfndlast_sht@5,
@`SORTfndlast_int@5, @`SORTfndlast_flt@5, @`SORTfndlast_lng@5,
@`SORTfndlast_dbl@5, @`SORTfndlast_loc@5 and @`SORTfndlast_var@5.

@h
/* type specific binary search implementations */
gdk_export BUN	SORTfnd_chr      (BAT *b, ptr v);
gdk_export BUN	SORTfnd_sht      (BAT *b, ptr v);
gdk_export BUN	SORTfnd_int      (BAT *b, ptr v);
gdk_export BUN	SORTfnd_flt      (BAT *b, ptr v);
gdk_export BUN	SORTfnd_lng      (BAT *b, ptr v);
gdk_export BUN	SORTfnd_dbl      (BAT *b, ptr v);
gdk_export BUN	SORTfnd_loc      (BAT *b, ptr v);
gdk_export BUN	SORTfnd_var      (BAT *b, ptr v);
gdk_export BUN	SORTfndfirst_chr (BAT *b, ptr v);
gdk_export BUN	SORTfndfirst_sht (BAT *b, ptr v);
gdk_export BUN	SORTfndfirst_int (BAT *b, ptr v);
gdk_export BUN	SORTfndfirst_flt (BAT *b, ptr v);
gdk_export BUN	SORTfndfirst_lng (BAT *b, ptr v);
gdk_export BUN	SORTfndfirst_dbl (BAT *b, ptr v);
gdk_export BUN	SORTfndfirst_loc (BAT *b, ptr v);
gdk_export BUN	SORTfndfirst_var (BAT *b, ptr v);
gdk_export BUN	SORTfndlast_chr  (BAT *b, ptr v);
gdk_export BUN	SORTfndlast_sht  (BAT *b, ptr v);
gdk_export BUN	SORTfndlast_int  (BAT *b, ptr v);
gdk_export BUN	SORTfndlast_flt  (BAT *b, ptr v);
gdk_export BUN	SORTfndlast_lng  (BAT *b, ptr v);
gdk_export BUN	SORTfndlast_dbl  (BAT *b, ptr v);
gdk_export BUN	SORTfndlast_loc  (BAT *b, ptr v);
gdk_export BUN	SORTfndlast_var  (BAT *b, ptr v);


@-
By Peter sept-99. This is a simple implementation that avoids all multiply
and divs on most bats by using integer BUNindex numbers rather than absolute
pointers (the BUNptr employed to obtain a pointer uses shift where possible).
Also, the gradient-based approach has been dropped again, which allows all
atoms to be treated in one macro. Main motivation: distrust of gradent
performance on odmg data and its high mult/div overhead.

@= SORTfnd
BUN SORTfndfirst_@2(BAT *b, ptr v) {
	BUN end = BUNfirst(b), cur = end;
	size_t lo = BUNindex(b,end), hi = BUNindex(b,BUNlast(b));
	int cmp = 1, bunsize = BUNsize(b);

	while (lo < hi) {
	    size_t mid = (lo+hi)>>1;
	    cur = BUNptr(b, mid);
	    cmp = @3_CMP(BUNt@1(b,cur),v,@4);
	    if (cmp < 0) {
		lo = ++mid; cur += bunsize;
	    } else if (cmp > 0) {
		hi = mid; 
	    } else {
		break;
	    }
  	}
	if (cmp == 0 && b->tkey == 0) {  /* shift over multiple equals */
	    while((cur-=bunsize) >= end) {
 		if (!@3_EQ(BUNt@1(b,cur),v,@4)) break;
	    } cur += bunsize;
	}
	return cur;
}

BUN SORTfndlast_@2(BAT *b, ptr v) {
	BUN end = BUNlast(b), cur = end;
	size_t lo = BUNindex(b,BUNfirst(b)), hi = BUNindex(b,end);
	int cmp = 1, bunsize = BUNsize(b);

	while (lo < hi) {
	    size_t mid = (lo+hi)>>1;
	    cur = BUNptr(b, mid);
	    cmp = @3_CMP(BUNt@1(b,cur),v,@4);
	    if (cmp < 0) {
		lo = ++mid; cur += bunsize;
	    } else if (cmp > 0) {
		hi = mid; 
	    } else {
		break;
	    }
  	}
	if (cmp == 0 && b->tkey == 0) {  /* shift over multiple equals */
	    while((cur+=bunsize) < end) {
 		if (!@3_EQ(BUNt@1(b,cur),v,@4)) break;
	    }
	} else if (cmp == 0) {
	    cur += bunsize;
	}
	return cur;
}

BUN SORTfnd_@2(BAT *m, ptr v) {
	BAT *b = BATmirror(m);
	size_t lo = BUNindex(b,BUNfirst(b)), hi = BUNindex(b,BUNlast(b));
	int cmp = 1, bunsize = BUNsize(b);
	BUN cur = NULL;

	while (lo < hi) {
	    size_t mid = (lo+hi)>>1;
	    cur = BUNptr(b, mid);
	    cmp = @3_CMP(BUNt@1(b,cur),v,@4);
	    if (cmp < 0) {
		lo = ++mid; cur += bunsize;
	    } else if (cmp > 0) {
		hi = mid; 
	    } else {
		break;
	    }
  	}
	return cmp?NULL:cur;
}

@= SORTfnd_switch
BUN SORTfnd@2(BAT *b, ptr v) {
	if (b && b->@1sorted)
	switch(ATOMstorage(b->@1type)) {
	case TYPE_chr:	return SORTfnd@2_chr(b,v);
	case TYPE_sht:	return SORTfnd@2_sht(b,v);
	case TYPE_int:	return SORTfnd@2_int(b,v);
	case TYPE_flt:	return SORTfnd@2_flt(b,v);
	case TYPE_dbl:	return SORTfnd@2_dbl(b,v);
	case TYPE_lng:	return SORTfnd@2_lng(b,v);
	default:
		if (b->@1varsized) {
			return SORTfnd@2_var(b,v);
		} else {
			return SORTfnd@2_loc(b,v);
		}
	}
	return NULL;
}
@c
@:SORTfnd(loc,chr,simple,chr)@
@:SORTfnd(loc,sht,simple,sht)@
@:SORTfnd(loc,int,simple,int)@
@:SORTfnd(loc,lng,simple,lng)@
@:SORTfnd(loc,flt,simple,flt)@
@:SORTfnd(loc,dbl,simple,dbl)@
@:SORTfnd(loc,loc,atom,b->ttype)@
@:SORTfnd(var,var,atom,b->ttype)@

@:SORTfnd_switch(h,)@
@:SORTfnd_switch(t,last)@
@:SORTfnd_switch(t,first)@

@+ User-defined Search Accelerators
@h
#define MAXACCS	24

#define ACCins(id, b, acc, idx, val)\
		if (id > ACC_index && BATaccelerators[id].accInsert) {\
			(*BATaccelerators[id].accInsert)(acc, idx, b, val);\
			b->haccdirty=TRUE;\
		}
#define ACCdel(id, b, acc, idx, val)\
		if (id > ACC_index && BATaccelerators[id].accDelete) {\
			(*BATaccelerators[id].accDelete)(acc, idx, b, val);\
			b->haccdirty=TRUE;\
		}
#define ACCmove(id, b, acc, from, to, val)\
		if (id > ACC_index && BATaccelerators[id].accMove) {\
			(*BATaccelerators[id].accMove)(acc, from, to, b, val);\
			b->haccdirty=TRUE;\
		}
#define ACCsave(id, b, acc)\
		if (id > ACC_index && BATaccelerators[id].accSave) {\
			(*BATaccelerators[id].accSave)(acc, b);\
			 b->haccdirty=TRUE;\
		} 
#define ACCcluster(id, b, acc)\
		if (id > ACC_index && BATaccelerators[id].accCluster) {\
			(*BATaccelerators[id].accCluster)(acc, b);\
			b->haccdirty=TRUE;\
		}
#define ACCbuncluster(id, b, acc)\
		if (id > ACC_index && BATaccelerators[id].accBuncluster) {\
      			(*BATaccelerators[id].accBuncluster)(acc, b);\
			b->dirtyBuns = TRUE;\
		}
#define ACCrepair(id, b, acc)\
		((id > ACC_index && BATaccelerators[id].accRepair)?\
      			(*BATaccelerators[id].accRepair)(acc, b):0)

gdk_export int ACC_hash;
gdk_export int ACC_index;

#endif /* _GDK_SEARCH_H_ */
@c
int ACC_hash = 1;
int ACC_index = 2;

int ACCbuild(int id, BAT *b, Heap* acc, ptr extra) {
	if (id == ACC_hash) {
		return BAThash(b, extra)?0:-1;
	} else if (id == ACC_index) {
		BAT *h = extra?BBPdescriptor(*(int*) extra):NULL;
		return BATidx(b, h)?0:-1;
	} else if (b->hacctype == id) {
		GDKwarning("ACCbuild: accelerator already present\n");
		return 0;
	} else if (VIEWparent(b)) {
		GDKwarning("ACCbuild: cannot create on a view BAT.\n");
		return 0;
	} else if (b->hacctype) {
		GDKwarning("ACCbuild: destroying %s first\n", ACCname(b->hacctype));
		ACCdestroy(b->hacctype, b, acc);
	} 
	if (BATaccelerators[id].accBuild) {
		(*BATaccelerators[id].accBuild)(acc, extra, b);
		b->hacctype = id;
                strcpy(b->haccname, BATaccelerators[id].name);
                b->batDirtydesc = b->haccdirty = TRUE;
		return 0;
	}
        GDKerror("ACCbuild: invalid accelerator '%s'.\n", ACCname(id));
	return -1;
}

int ACCunload(int id, BAT *b, Heap *acc) {
	if (id == ACC_hash) {
		HASHremove(b);
	} else if (id == ACC_index) {
		IDXremove(b);
	} else if (b->hacctype != id) {
        	GDKerror("ACCunload: invalid accelerator '%s'.\n",ACCname(id));
        	return -1;
	} else if (VIEWparent(b)) {
		GDKwarning("ACCunload: cannot destroy on a view BAT.\n");
	} else if (BATaccelerators[id].accDestroy) { 
		(*BATaccelerators[id].accDestroy)(acc, b);
	}
	return 0;
}

int ACCdestroy(int id, BAT* b, Heap* acc){
	int ret = 0;
	if (b->haccCopiedtodisk) {
		str nme = BBP_physical(b->batCacheid);
		HEAPdelete(&b->haccelerator, nme, "hacc");
		b->haccCopiedtodisk = 0;
        	b->batDirtydesc = TRUE;
	}
	if (id > ACC_index) {
		ret = ACCunload(id, b, acc);
		BATmirror(b)->tacctype = b->hacctype = 0;
        	b->haccname[0] = 0;
        	b->batDirtydesc = TRUE;
	}
	return ret;
}


int ACCrepairSTD(Heap *hp, BAT *b) {
	HeapRepair hr;
	int ret = HEAP_check(hp, &hr);

	if (hr.validmask) {
		GDKfree(hr.validmask);
		hr.validmask = NULL;
	}
	if (ret == FALSE) {
		GDKerror("ACCrepairSTD(%s): check failed, destroying accelerator.\n", b->batId);
		ACCdestroy(b->hacctype, b, hp);
	}
	return ret;
}

void ACCunloadall(BAT *b) {
	IDXdestroy(b);
	HASHdestroy(b);
        if (b->hacctype) {
                ACCunload(b->hacctype, b, &b->haccelerator);
        }
        if (b->tacctype) {
                ACCunload(b->tacctype, BATmirror(b), &b->taccelerator);
        }
}

void ACCremoveall(BAT *b) {
	if (VIEWparent(b)) {
		return;
	}
	IDXdestroy(b);
	HASHdestroy(b);
        if (b->hacctype && b->haccelerator.base) {
                ACCdestroy(b->hacctype, b, &b->haccelerator);
        }
        if (b->tacctype && b->taccelerator.base) {
                ACCdestroy(b->tacctype, BATmirror(b), &b->taccelerator);
        }
}

accDesc BATaccelerators[MAXACCS] = {
   { "_dummy_",	0,		0,		0,
		0,		0,		0,
		0,		0,		0,
		0,		0,		0 },
   { "hash",	0,		0,		0,
		0,		0,		0,
		0,		0,		0,
		0,		0,		0 },
   { "index",	0,		0,		0,
		0,		0,		0,
		0,		0,		0,
		0,		0,		0 } };






int	GDKacc_cnt = 3;

int ACCindex(str name) {
	int i, j = GDKacc_cnt;

	if (strcmp(name, "hash") == 0) {
		return ACC_hash;
	} else if (strcmp(name, "index") == 0) {
		return ACC_index;
	}
	for(i = 1; i < GDKacc_cnt; i++) {
		if (BATaccelerators[i].name[0]) {
			if (strcmp(BATaccelerators[i].name, name) == 0) {
				return i;
			}
		} else if (j == GDKacc_cnt) {
			j = i;
		}
	}
        return -j;      
}

void ACCdelete(int i) {
	if (i >= GDKacc_cnt-1) {
		GDKacc_cnt--;
	}
	BATaccelerators[i].name[0] = 0;
}

void ACCproperty(str name, str prop, ptr fcn) {
	int i;
	MT_set_lock(GDKthreadLock, "ACCproperty");
	i = ACCindex(name);
	if (i < 0) {
		i = -i;
		strcpy(BATaccelerators[i].name, name);
		BATaccelerators[i].deleting = 1;
		if (i >= GDKacc_cnt) {
			GDKacc_cnt++;
		}
	}
	if (strcmp(prop, "build") == 0) {
		BATaccelerators[i].accBuild = (void (*)(Heap*, BAT*, void *param)) fcn;
	} else if (strcmp(prop, "destroy") == 0) {
		BATaccelerators[i].accDestroy = (void (*)(Heap*, BAT*)) fcn;
	} else if (strcmp(prop, "insert") == 0) {
		BATaccelerators[i].accInsert = (void (*)(Heap*, size_t, BAT*, ptr)) fcn;
	} else if (strcmp(prop, "delete") == 0) {
		BATaccelerators[i].accDelete = (void (*)(Heap*, size_t, BAT*, ptr)) fcn;
	} else if (strcmp(prop, "move") == 0) {
		BATaccelerators[i].accMove = (void (*)(Heap*, size_t, size_t, BAT*, ptr)) fcn;
	} else if (strcmp(prop, "save") == 0) {
		BATaccelerators[i].accSave = (void (*)(Heap*, BAT*)) fcn;
	} else if (strcmp(prop, "commit") == 0) {
		BATaccelerators[i].accCommit = (void (*)(Heap*, BAT*)) fcn;
	} else if (strcmp(prop, "abort") == 0) {
		BATaccelerators[i].accAbort = (void (*)(Heap*, BAT*)) fcn;
	} else if (strcmp(prop, "cluster") == 0) {
		BATaccelerators[i].accBuncluster = (int (*)(Heap*, BAT*)) fcn;
	} else if (strcmp(prop, "repair") == 0) {
		BATaccelerators[i].accRepair = (int (*)(Heap*, BAT*)) fcn;
	}
        MT_unset_lock(GDKthreadLock, "ACCproperty");
}

char *ACCname(int t) {
	return  (t>0 && t<GDKacc_cnt && BATaccelerators[t].name)?
                               BATaccelerators[t].name:"null";
}
