% -*-LaTeX-*-
\documentclass[10pt,twocolumn,fleqn]{article}

%%\usepackage{a4wide}
%%\usepackage{amsthm}
\usepackage{times}
\usepackage{epsf}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{color}
\usepackage{url}

\begin{document}

\title{The Vulcano execution model in Monet 5.0}
\author{ Martin Kersten,{\small \textsc{CWI}, Netherlands}}
\date{}
\maketitle

\section{Introduction}
The x100 execution model is characterised by with the following properties:
\begin{itemize}
\item {\em pipelined execution}, the query execution is data driven using 
small, cache-optimized pipeline buffers and algorithms
I.e. chunking BATS to fit the storage area.
\item{\em nsm aware processing}, the attributes necessary to evaluate a 
query term are loaded together (pre-processing/post-processing)
I.e operators defined over multiple chunks instead of a single BAT or BUN.
\item{\em minimal intermediate storage}, expression evaluation respects 
the limited size of the cache to avoid swapping temporary results and
to re-use scarce space where possible.
I.e. an accumulator evaluation model at the level of chunks.
\end{itemize}

Since most relational tables are far larger then the available cache memory, 
it becomes mandatory to fragment them upfront. During this process, 
one can re-arrange the data in such a way that subsequent expression 
evaluation becomes cheap (cheaper)

The fragmentation can be static or dynamic. The former takes the standpoint
to administer fragmentation and make it visible to the outside world.
The latter is intertwinned with the query evaluation.The base tables can be
fragmented multiple times with different criteria.

The two complementary operators for this process within Monet context are
\begin{itemize}
\item {\em cluster}, which performs a RADIX cluster over an operand
\item{\em decluster}, which performs a RADIX decluster
\end{itemize}

For a query execution tree (de-)cluster merely becomes an additional node, 
which produces a sequences of clusters, rather then individual tuples.
This process is not really new.

\subsection{TEC and PODERE}
The Total Execution Cost (TEC) is what an end-user of application program will
notice. The TEC is composed on several cost factors: 
\begin{itemize}
\item - Parsing
\item - Optimization and code generation
\item - Data access to the operands
\item - Execution of the query term,
\item- Result construction 
\item - and Emission of the result to the application
\end{itemize}
Choosing architecture for processing database queries pre-supposes an
intuition on where and how the cost will be distributed. In an OLTP
setting you expect most of the cost to be in PO, while in OLAP it will
be DER. In a distributed setting the components ODE are dominant.
Web-applications should focus focus on E.

However, such a simple first characterization ignores the wide-spread
differences that can be experienced at each level. To illustrate,
in D and R is makes a big difference whether the data is already in the
cache of still on disk. Within E it makes a big difference whether you
are comparing 2 integers, performing an math function such as Log, or
a regular expression evaluation on a string.
As a result, intense optimization in one area may become completely invisible
due to other cost factors..

This means that in picking an architecture one should avoid the pitfall of
penny-wise pound foolishness. Unfortunately, this can onlye determined a
posteriori by observing real users of the system. The worst that can happen
is that a complex query can be processed efficiently, while the user
hangs on a slow emission (i.e.network connectivity).

An architecture to support PODERE should provide different avenues to
reach a satisfactory goal. This depends on a user with proper articulation
of his tasks (e.g. array based processing or k-NN feature access), and
a system infrastructure to match this with the proper tools.

It seems that the x100 module is currently too strongly focussed on DER
with a very simplistic expression evaluation. (X+Y is 5 times faster as
log(X) and can be 100 times faster as like(url,".*/[a-z0-]*.jpg).
Moreover, it is biased to a uniform distribution. In high-selectivity queries
it makes sense to use the maximal buffer to prune the candidate set before
accessing related attributes.
\subsection{Push/pull model}
The choice of a pipelined execution model calls for a decision on who is in
control. The two extreme positions are push/pull. In a push context all
operations are considered running in parallel filling a pipeline segment.
It is the preferred choice when the query is represented as a graph and it is
a priori unclear what part can produced partial answers first. Furthermore, 
it works best in a parallel setting. Complications encountered are the
synchronization points, i.e. a operator should wait until all its parameters
have been instanciated.

In a pull model, operators call the sub-expressions to deliver some partial
results. This translates to a co-routine structure, which can be 
repeatedly called to obtain the next granule.
The advantage over {\em push} model is that an optimizer can schedule 
access to the components and intervene when sub-expression evaluation turns
out to allow for a short exit route. The disadvantage is that in a slow 
memory setting (disks), you don;t know the proper order of reading.

Both schemes have to choose the granule of interaction, being it a tuple,
a buffer with tuples, or a collection of BATfragments.

\section{Monet 5 model}
A pipelined execution model complements the approach taken in M4.
It had been foreseen for a long time that some pipelined scheme would be needed.
The main questions are the level of granularity,
the pipe line representation, and the corresponding interpreter structure.

A pipelined execution model essentially means a stream processing model.
A stream model consists of three phases Initialize(), Next(), Finalize();

Within M 5, the control structures for processing streams and iterators is
cast in the following unifying structure.
\begin{verbatim}
barrier  B:= Init(Params)
	... code to process an expression ...
redo	B:= Next(Params)
exit	B:= Final(Params);
\end{verbatim}

The (stream) iterator is represented by {\em Params}. The block is 
entered when the
initialization succeeds and is left when calling for the next block fails.
It is allowed to intersperse code beteen the {\em redo} and {\em exit} 
to post-process the result variables before leaving.
Likewise, it is allowed to include a {\em leave} statement at any place to
exit the iterator.

This mechanism permits a tree-based execution scheme. The approach represented
in the x100 example merely leads to inclusion of the parallel initialization
of the vectors. For example:
\begin{verbatim}
	initScanProperties(memchunk,iochunk);
barrier B1:= initScan(l_returnflag,pos1)
	B2:= initScan(l_linestatus,pos2)
	... code to process an expression ...
	B2:= nextScan(l_linestatus,pos2)
redo	B1:= nextScan(l_returnflag,pos1)

	B2:= finalScan(l_linestatus,pos2);
exit	B1:= finalScan(l_returnflag,pos1);
\end{verbatim}

The routine {\em initScan} looks up the information on the return flag
column and initializes the 'read' position.
Once they are available, the vector-based operator can be called, e.g.
x100.select(Res,B1,..,Bn) where Res is a result vector allocated outside
the loop. (Side issue, given the diversity of the underlying datatypes (STR) in
Monet, this vector should share the heaps with the BATS or create
a new one. This may lead to excessive overheads)

\section{Push based processing}
The current MAL interpreted is pull based. Each instruction is executed before
the next instruction is taken into consideration. To turn the interpreted into
a push-based version requires rewriting the interpreter and the
underlying routines to be re-entrant.

A step into this direction is to cast a routine into a separate thread, which
contains the following structure.
\begin{verbatim}
thread  x100.select(Res:Vector[int], B1:Vector[int],...)
	# initialize the result vector
barrier B1:= reentrantCall(Res,B1,...,Bn);
	... code to process an expression ...
	return Res;
redo	B1;
exit	B1;
	#remove any garbage
end	x100.select;
\end{verbatim}
The interpreter can recognize a call to a thread and schedule transfer
of control, i.e. queue the request. The routine {\em reentrantCall} merely
inspects the queue of pending results and switches the execution context
accordingly, i.e. taking control over the stack frames to access the
operands and to deliver a result.


\section{Design considerations for M7}
\subsection{Monet in the community}

Monet will be used within the community, we will be obliged to
support derivations of 4.3 for at least 5 years. (DD link)
Furhtermore, we haven;t even started to conquer the world with
an already open and fast subsystem. We should focus on broadening
the client based first. 

\subsection{What is the next application space}
Still we haven;t dealt with proper with multimedia support and GIS.
The foreseable future calls for database support involving large
array based processing, IR, P2P, and ambient interaction.

\subsection{Why pipleine}
\begin{itemize}
\item the result of an expression is limited in size; the cluster granule 
is derived from the cache properties rather then an arbitrary sized bat, 
swapped to memory/disk.
\item results are produced in a pipelined fashion, providing early response 
to the user of partial results. (provided it does not contain a 
blocking operator)
\item all columns relevant for a query subexpression are handled together,
which 're-established' an NSM view over a series of BATs.
\end{itemize}

\subsection{Why to replace GDK}
There are several reasons to reconsider major portions of GDK.
Most of which has to do with propagation of language, interpreter level
issues into the kernel (eg reference counting and session persistency).
Likewise, it lacks a proper catalog structure with a clear method for
handling properties. Finally, a gateway structure is needed to interact
and handle foreign formats easily.

Functional, the BATs are one point in the design space. You can imagine
BATs without any storage, but merely representing a (inverseable) function.
Also the mechanism to attach triggers is underdeveloped.

\subsection{Why to use a query graph interface}
The choice to replace GDK with an engine that accepts a query graph and
under the cover performs a standard execution strategy seem contrary to
our experiences. Rather then making more of the detailed information visible
at the engine interface layer (in terms of properties), we hide information.

Second, not all queries are best supported by stream based processing. Most
queries in practice are rather mundane OLTP type, which access a few records
by key. This facet also appears if you anticipate multiple language 
front-ends

Third, taking into account all kind of properties, the engine becomes a huge
beast, or you end-up performing too many  administrative test again at the
lowest level of the system. To avoid it, we need a few thousand Init/Next/Final
routines, each optimized for the right type, access paths, contraints (
uniqueness, non-null), possible exceptions being raised (/0)

Fourth, it is not a priori clear that you should perform a parallel read
on the columns of interest. Thist strongly depends on the result
produced by an operator. For example, consider a table R(A,B) with a
skewed value distribution over A, which also happens to be clustered.
Then, it makes more sense first to scan A completely before loading the
relevant clusters of B.

Fifth, you need a canonical, easy to interpret representation to make a
step into the direction of multi-query optimization or even memorization.

Six, how does vectorization differ from pipelined parallel execution with
vectors as the unit of manipulation. Doesn;t it mean that once you are
able to describe and interpret a PP distributed graph you have the
local pipeline approach for free.
\end{document}
