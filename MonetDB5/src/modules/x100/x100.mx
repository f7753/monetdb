@f x100
@a Peter Boncz, Niels Nes
@v 0.1 
@t TIMES 100: the future of query-intensive database kernels developed at CWI
@T
x100 is a database kernel that implements a standard relaional query algebra (scan,
select, project, aggr / todo: sort, join) using a volcano-like iterator
model. Its main divergent characteristics are:
\begin{itemize}
\item its execution primitives process value vectors iso one tuple at-a-time.
\item it uses vertical decomposition throughout. (TODO:) Disk storage are compressed 
  projection indices, maintained under updates using differntial lists. 
\item (TODO:) it tunes memory access: keeps random patterns inside the cpu-cache and boosts 
  sequential access using a hardware abstraction layer for fast sequential transfers.
\end{itemize}

One could say that vectorized processing is similar to (multiplex) MIL processing.
While this is true, the crucial difference is that x100 is not penalized for
materializing result vectors, since vector sizes are variable and typically small, such
that all intermediate result vectors of an active query graph can be tuned to fit 
the memory caches). As such, it does away with many existing Monet limitations in 
terms of bandwidth wastage, among others with Monet's inability to efficiently execute 
non-hardcoded multi-column expressions (such as compound selection conditions).

The challenge in the x100 system is to decompose non-trivial query processing
algorithms in vectorized versions. Specifically, this means that program logic
inside if-then-else blocks, as well as while loops, that normally appear in query
processing algorithms (such as if-testing for hashing matches or while-looping over
a collision list) should be massaged out of the vectorized loop. All basic
steps should be taken on multiple tuples at-a-time, replacing if-testing by
vector size reduction (for this purpose there is basic support for indirect
value adressing: using an index vector to point to selected slots in a bigger 
data vector) and replacing loops on tuples by loops on vectors.

Early experiments on Itanium2 machines have shown that vectorized execution on TPC-H
queries is 10 times faster than Monet (and 100 times faster than MySQL,BD2,Oracle)
an can even beat hand/hard-coded tuple-at-a-time iterator query plans. The main reason
for this is that branch preditions can be fully eliminated and the code has a very high
degree of independence, such that a wide-issue machine can get to its optimal IPC (>3!).

This code should be regarded as a preliminary prototype. In a future, we should
\begin{itemize}
\item rewrite: make the code standalone, convert to C++, split in various files, use m4 iso Mx.
\item complete the functionality (varsized types, vector property propagation, sort, join)
\item maybe even adapt the SQL frontend to also generate this algebra
\item maybe even write a MIL executor on top of x100
\item maybe even add transaction functionality
\end{itemize}
There is a hardcoded test, that does TPC-H q1. It builds the query processing tree internally, 
and executes it.
@c
#define _XOPEN_SOURCE	600

#include "uchr.h"
#include "monettime.h"
#include "x100_engine.h"
#include "x100.h"
#include <stdlib.h>

#if defined(ia64) && !defined(__GNUC__)
#include <xmmintrin.h>
#include <ia64intrin.h>		/* __lfetch */
#endif

#define ALIGN 16
#define memalign(n,m) ((n+m-1)&(~(m-1)))

void *MALLOC(int size){
	void *res = NULL;
	res = GDKmalloc(size);
	assert(res);		  /* couldn't allocate realsize */
	assert(!((long)res & 7)); /* should have been 15 but this fails within monet */
	return res;
}

void* ZALLOC(size_t n) {
	str p = (str) MALLOC(n);
	memset(p, 0, n);
	return (void*) p;
}

void FATAL(char *format, ...) {
        va_list ap;
        va_start(ap, format);
        vfprintf(stderr, format, ap);
        va_end(ap);
	assert(0);
}
@h
#define STRDUP(x) GDKstrdup(x)	
#define FREE(x) GDKfree(x)	

@* Type System
@T
\begin{verbatim}
uidx = unsigned int of the natural length (32-bits on 4 byte OS, 8-byte on 64-bits)
      #tuples in a table and counts return thus uidxs.
uint = unsigned 32 bits integer
uchr = unsigned byte

we use uint here as boolean type as we assume that bytes will be more expensive to access from the cache.
\end{verbatim}
@h
typedef struct {
	char *name;
	size_t width;
	uint cardinal;
	size_t (*string)(void *v, str buf, size_t len); /* return byte length actually needed in buffer; 0 means error */
	size_t (*value)(str s, void *buf, size_t len); /* return byte length actually needed in string; 0 means error */
} Type;

@= conv
size_t @1Value(str s, void* buf, size_t len) {
	if (len >= sizeof(@1)) {
		int ret = sscanf(s, @2, buf);
		if (ret <= 0) return 0; /* parse error */
	}
	return sizeof(@1);
}

size_t @1String(void *p, str buf, size_t len) {
	return (size_t) snprintf(buf, len, @2, *(@1*) p);
}
@c
@:conv(uchr,"%c")@
@:conv(schr,"%c")@
@:conv(usht,"%u")@
@:conv(ssht,"%d")@
@:conv(uint,"%u")@
@:conv(sint,"%d")@
@:conv(ulng,"%llu")@
@:conv(slng,"%lld")@
@:conv(flt,"%f")@
@:conv(dbl,"%lf")@
@:conv(uidx,sizeof(uidx)==sizeof(uint)?"%u":"%lu")@

Type x100_types[] = { 
	{ "uchr", sizeof(uchr), TRUE, uchrString, uchrValue, }, 
	{ "schr", sizeof(schr), TRUE, schrString, schrValue, }, 
	{ "usht", sizeof(usht), TRUE, ushtString, ushtValue, }, 
	{ "ssht", sizeof(ssht), TRUE, sshtString, sshtValue, }, 
	{ "uint", sizeof(uint), TRUE, uintString, uintValue, }, 
	{ "sint", sizeof(sint), TRUE, sintString, sintValue, }, 
	{ "ulng", sizeof(ulng), TRUE, ulngString, ulngValue, },
	{ "slng", sizeof(slng), TRUE, slngString, slngValue, },
	{ "flt",  sizeof(flt),  FALSE, fltString, fltValue,  }, 
	{ "dbl",  sizeof(dbl),  FALSE, dblString, dblValue,  }, 
	{ "uidx", sizeof(uidx), TRUE, uidxString, uidxValue, }, 
	{ NULL, 0, 0, 0, 0, },
};

Type* GetType(str name) {
	int i = -1;
	while(x100_types[++i].name) 
		if (strcmp(x100_types[i].name,name) == 0) return x100_types+i;
	FATAL("GetType: type %s not found\n", name);
	return NULL;
} 

/* map a Monet void BAT into an x100 column (if possible) */ 
Type* GetBatType(BAT* b, char* fcn) {
	char* tpe = NULL;
	if (b && b->htype == TYPE_void && !b->tvarsized && ATOMsize(b->ttype) == BUNsize(b)) {
		int t = ATOMstorage(b->ttype);
		if (b->ttype == TYPE_chr || b->ttype == TYPE_uchr) {
			return GetType("uchr");
		} else if (b->ttype == TYPE_int || b->ttype == TYPE_date) {
			return GetType("uint");
		} else if (b->ttype == TYPE_sht) {
			return GetType("usht");
		} else if (b->ttype == TYPE_dbl) {
			return GetType("dbl");
		} else if (b->ttype == TYPE_flt) {
			return GetType("flt");
		}
	}
	FATAL("%s: %s had the wrong types [%s,%s] or width %d.\n", 
		fcn, BBP_logical(b->batCacheid), ATOMname(b->htype), ATOMname(b->ttype), BUNsize(b));
	return NULL;
}

@+ Function Lookup Table
@T
This is a basic lookup table where functions are encoded by their signature: primitive_op_{tpe_param}
\begin{verbatim}
where:
- primitive = "map", "select" or "aggr"
- op        = the SQL/algebra function name (may be a functor symbol like ">>" or "&")
- tpe       = a x100 base type name (e.g. "uchr")
- param     = "col" (a vector parameter) or "val" (a constant parameter)
\end{verbatim}
the lookup table provides access given such a signature to the C implementation routine address, and the return type (as a string).
@h
typedef size_t (*ExprFcn)(size_t n, void* res, void* param0, void* param1, void* param2, void* param3, void* param4, void* param5, void* param6, void* param7, void* param8, void* param9, void* sel);

typedef struct {
	str returntype, fcnnme; 
	ExprFcn fcn;
} FcnMap;

extern FcnMap x100_fcnmap[];
@c
FcnMap *GetFcnMap(str name) {
	char lowername[1024];
	int i;
	for(i=0; name[i] && i<1024; i++) 
		lowername[i]=tolower(name[i]);
	lowername[i]=0;
	for(i=0; x100_fcnmap[i].fcnnme; i++) 
		if (strcmp(x100_fcnmap[i].fcnnme,lowername) == 0) return x100_fcnmap+i;
	FATAL("GetFcnMap: signature %s not found\n", name);
	return NULL;
}

@* Vectors

Vectors are comparable with unary (void-head) BATs from Monet. 
TODO: intelligently and efficiently incorporate variable-sized atoms
@h
typedef struct {
	size_t width, size, maxsize; /* allocated = width*maxsize, used = width*size */
	void *ptr, *base, *first; /* array of fixed-size values, ptr = allocated pointer, base = aligned base address, first = current start */
	int refcnt; /* for garbage collection: #FcnExpr that refer to this */ 
} Vector;

@c
Vector *VectorAlloc(Type *tpe, size_t size) {
	Vector *this = (Vector*) ZALLOC(sizeof(Vector));
	this->width = tpe->width; 
	this->maxsize = this->size = size;
	this->ptr = MALLOC(this->width*size + ALIGN);
	this->base = 
	this->first = (void*)memalign(((size_t)this->ptr), ALIGN);
	return this;
} 

Vector *VectorConst(Type *tpe, void *value) {
	Vector *this = (Vector*) ZALLOC(sizeof(Vector));
	this->size = 1;
	this->width = tpe->width;
	if (value) {
		this->ptr = MALLOC(tpe->width + ALIGN);
		this->base = 
		this->first = (void*)memalign(((size_t)this->ptr), ALIGN);
		memcpy(this->base, value, tpe->width);
	}
	return this;
}

void VectorRealloc(Vector *this, Type* tpe, size_t maxsize) {
        void *ptr = MALLOC(this->width*maxsize + ALIGN);
	void *base = (void*)memalign(((size_t)ptr), ALIGN);
	size_t diff = (char*)(this->first) - (char*)(this->base);
        memcpy(base, this->base, this->width*this->maxsize);
        FREE(this->ptr);
        this->ptr = ptr;
        this->base = base;
	this->first = base; /*(void*)((char*)(this->base) + diff);*/
	this->maxsize = maxsize;
}

void VectorExtend(Vector *this, Type *tpe, size_t inc) {
	if (((str) this->first)+this->size+inc <= ((str) this->base)+this->maxsize) {
		VectorRealloc(this, tpe, this->maxsize+MAX(this->maxsize/2,inc));
	}
	this->size += inc;
}

void VectorFree(Vector *this) {
	if (--this->refcnt <= 0) { 
		if (this->ptr) FREE(this->ptr);
		FREE(this);
	}
}

@* Expressions
@T
Expressions may be either constants, column references or functions, which have zero or more expressions 
as parameter.  Attached each (non-constant) expression is a result vector, that contains the result
of evaluating the expresion on a number of tuples. Constant expressions also store their single constant
in a vector of length 1. 

As expressions are parts of directed acyclic graphs, they may be referenced multiple times, so they have 
a reference count for garbage collection purposes. To save memory, parent expressions may actually use
a child result vector as their result vector. Thus, vectors (see above) also have their individual 
reference counts.

Functor expressions are resolved at some time, which means that their children are resolved, such that
all parameter result types are known, such that its own result type can be derived. At that time, the
result vector can actually be allocated (as then the vector width becomes known), and the reference
to the engine function can be resolved.

We now use a list over properties to only store the relevant properties for an expression, and to make the 
collection of possible properties dynamic instead of hardcoded.

@h
#define NPARAMS 10

typedef struct Expr_ {
	int evalid; /* magic id for executing shared expressions in graphs only once */
	Vector *vec; /* FcnExpr result value chunk. ColExpr vector points right in BAT. */
	Type *tpe; /* type of ->vec values */   
	str name; /* logical name for expression */
	int refcnt; /* for garbage collection: #Operators plus #FcnExpr that refer to this */ 
	int reuse; /* true iff the vector buffer is reused by some parent expression */
	void* prefetch; /* true iff prefetching is needed */

	/* FcnExpr state: function and its parameters */
	ExprFcn fcn; /* resolved operator */
	str fcnname; /* operator name as string */
#ifdef PROFILE
	ulng pcnt;
#endif
	struct Expr_ *params[NPARAMS+1]; /* list terminated with a NULL Expr */
	void *properties; /* list of properties */
} Expr;

#define ISCONSTEXPR(this) (this->vec->ptr && !this->params[0]) 
@c
Expr *ConstExpr(void* value, Type* tpe) {
	Expr *this = (Expr*) ZALLOC(sizeof(Expr));
	this->tpe = tpe;
	this->vec = VectorConst(tpe, value);
	this->vec->refcnt++;
	return this;
}

Expr *FcnExpr(str fcn, Expr **params) {
	Expr *this = (Expr*) ZALLOC(sizeof(Expr));
	int i;
	for(i=0; params[i]; i++) {
		this->params[i] = params[i];
		params[i]->refcnt++;
	} 
	this->fcnname = fcn;
	return this;
}

static void ResolveFcn(Expr *this, str prefix, size_t n) {
	char signature[1024];
	FcnMap *m;
	int i, j;
	if (this->params[0] == NULL || this->vec != NULL || this->fcn) 
		return;
	for(i=0; this->params[i]; i++) {
		ResolveFcn(this->params[i], "map", n);
		if (this->params[i]->reuse) {
			/* too early sharing detected!! must undo it */
			Expr *e = this->params[i];
			Vector *v = e->vec, *c = VectorAlloc(this->params[i]->tpe, v->maxsize);
			e->reuse = 0;
			while(e) {
				v->refcnt--;
				this->params[i]->vec = c;
				c->refcnt++;
				for(j=0; e->params[i]; j++) 
					if (e->params[i]->vec == v) break;
			}
		}
	}
	snprintf(signature, 1024, "%s_%s", prefix, this->fcnname);
	for(i=0; this->params[i]; i++) {
		strncat(signature, "_", 1024);
		strncat(signature, this->params[i]->tpe->name, 1024);  
		strncat(signature, ISCONSTEXPR(this->params[i])?"_val":"_col", 1024);
	} 
	m = GetFcnMap(signature);	
	this->fcn = m->fcn; 
	this->tpe = GetType(m->returntype); 
	this->vec = VectorAlloc(this->tpe, n);

	if (strncmp("aggr", prefix, 4) == 0)
		memset(this->vec->first, 0, n*this->vec->width);

	/* try to share vector from child with same type */
	for(i=0; this->params[i]; i++) {
		if (strncmp(prefix, "aggr",4) && 
		    this->params[i]->fcnname && 
		    this->params[i]->refcnt <= 1 && 
		    this->params[i]->vec->width >= this->vec->width) 
		{
			VectorFree(this->vec);
			this->vec = this->params[i]->vec;
			this->params[i]->reuse = 1;
		}
	}
	this->vec->refcnt++;
}


@- Properties
Properties can be attached to either expressions or entire tables (operators). They have a name, and
some value that is a vector (so it can be either a singular value or something more complex).
@h
typedef struct _Property {
	str name;
	Type* tpe;
	Vector* val;
	struct _Property *next;
} Property;
@c
Vector* GetProperty(void *prop, str name) {
	Property *cur;
	for(cur = (Property*) prop; cur; cur=cur->next) 
		if (strcmp(cur->name, name) == 0) return cur->val;
	return NULL;
}

Expr* SetProperty(Expr *this, str name, Type *tpe, Vector *val) {
	Property *prop = (Property*) ZALLOC(sizeof(Property));
	prop->name = STRDUP(name);
	prop->tpe = tpe;
	prop->val = val;
	prop->next = this->properties;
	val->refcnt++;
	this->properties = (void*) prop; 
	return this;
}

Property *FreeProperty(Property *this) {
	Property *next = this->next;
	if (this->name) FREE(this->name);
	if (this->val) VectorFree(this->val);
	FREE(this);
	return next;
}


@+ Prefetching
Seems hardware prefetching isn't up to its take on most platforms (P4 being the exception). Therefore we implement
software prefetching optimized per platform. A default implementation just sums int with cacheline strides.
For AMD Athlon systems backward read of 'movl's is optimal (see www.amd.com).  For systems without hardware 
prefetching, a software prefetch is implemented, ie a simple prefetch loop.

@c
#if defined(ATHLON) 
static inline int Mem2Cache( void *from, int size )
{
	int cnt = size/128;
	__asm__ __volatile__ (
		"movl %2, %%eax\n\t"
		"movl %0, %%esi\n\t"
		"addl %1, %%esi\n\t"
		"1: movl -128(%%esi), %%edx\n\t"
		"movl -64(%%esi), %%edx\n\t"
		"subl $128, %%esi\n\t"
		"dec %%eax\n\t"
		"jnz 1b\n\t"
                :  : "r" (from), "r" (size), "r" (cnt) : "memory",  "%eax", "%edx", "%esi" );
	return cnt;
}
#else

#if defined(ia64)
/* L2 has 128 byte cachelines */
static inline int Mem2Cache( void *from, int size )
{
	char *data = from;
	int i;
	for(i=0; i<size; i += 64){
#ifndef __GNUC__
        	_mm_prefetch(data, _MM_HINT_NTA );
#else
        	__builtin_prefetch(data);
#endif
		data += 64;
	}
	return i;
}
#else

#if defined(PREFETCH)

#define CACHELINE (64)
#define INTCACHELINE (CACHELINE/sizeof(int))

static inline int Mem2Cache( int *p, size_t size){
	size_t sum0 = 0, sum1 = 0, i = 0;
	size /= sizeof(int);

	while(i+(8*INTCACHELINE) < size){
		sum0 += p[i+0*INTCACHELINE];
		sum1 += p[i+1*INTCACHELINE];
		sum0 += p[i+2*INTCACHELINE];
		sum1 += p[i+3*INTCACHELINE];
		sum0 += p[i+4*INTCACHELINE];
		sum1 += p[i+5*INTCACHELINE];
		sum0 += p[i+6*INTCACHELINE];
		sum1 += p[i+7*INTCACHELINE];
		i += INTCACHELINE*8;
	}
	return sum0+sum1;
}

#else

#define Mem2Cache(p,size)

#endif
#endif
#endif

#ifdef PROFILE
ulng x100_clock(){
	unsigned long long tsc;
#if defined(i386)
	__asm__ __volatile__("rdtsc" : "=A" (tsc));
#elif !defined(ia64) 
	tsc = (unsigned long long) clock();
#elif defined(__GNUC__)
	__asm__ __volatile__("mov %0=ar.itc" : "=r"(tsc) :: "memory");
#else
	tsc = (unsigned long long) __getReg(_IA64_REG_AR_ITC);
#endif
	return tsc;
}

void profileExpr(Expr *this, char *ident){
	char buf[BUFSIZ];
	int i;

	if (!this->evalid)
		return;
	
	this->evalid = 0;
	strcpy(buf, ident);
	strcat(buf, "\t");

	for (i=0; this->params[i]; i++)
		profileExpr(this->params[i], buf);

	if (this->fcnname)
		printf("%s%s %lld\n", ident, this->fcnname, this->pcnt );
}

void profileOperator(Operator *this, char *ident){
	char buf[BUFSIZ];
	int i;

	strcpy(buf, ident);
	strcat(buf, "\t");

	if (this->son != NULL && this->son != this)
		profileOperator(this->son, buf);
	for(i=0; this->expr[i]; i++)
		profileExpr(this->expr[i], ident);
	if (this->direct_group)
		profileExpr(this->direct_group, ident);
	if (this->groupid)
		profileExpr(this->groupid, ident);
	if (this->hashnum)
		profileExpr(this->hashnum, ident);
	for(i=0; i<this->ngroupbys; i++)
		profileExpr(this->groupby[i], ident);
}
#else
#define profileOperator(this, ident)
#endif

size_t ExprEval(Expr *this, size_t n, Vector *sel, int evalid) {
	if (this->params[0] && evalid != this->evalid) {
		void* p[NPARAMS+1];
		int i;
		for(i=0; this->params[i]; i++) {
			(void) ExprEval(this->params[i], n, sel, evalid);
			p[i] = this->params[i]->vec->first;
			if (this->params[i]->prefetch){
				Mem2Cache(this->params[i]->prefetch, this->params[i]->vec->size*this->params[i]->vec->width);
				this->params[i]->prefetch = NULL;
			}
		}
		p[i] = sel?sel->first:NULL;
#ifdef PROFILE
		{ ulng profiling = x100_clock();
#endif
		n = this->fcn(n, this->vec->first, p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7], p[8], p[9], p[NPARAMS]);  
#ifdef PROFILE
		profiling = x100_clock() - profiling;
		this->pcnt += profiling;
		}
#endif

	}
	this->evalid = evalid;
	return n;
}

void ExprFree(Expr *this) {
	if (--this->refcnt <= 0) {
		int i;
		Property *p = (Property*) this->properties;
		while(p) p = FreeProperty(p);
		for(i=0; this->params[i]; i++) {
			ExprFree(this->params[i]);
		}
		VectorFree(this->vec);
		if (this->name) FREE(this->name);
		FREE(this);
	}
} 

Expr *SetName(Expr *this, str name) {
	if (this->name) FREE(this->name);
	this->name = STRDUP(name);
	return this;
}

str GetName(Expr *this, str buf, size_t len) {
	if (this->name) {
		strncat(buf, this->name, len);
	} else if (this->fcnname) { 
		int i, l;
		snprintf(buf, len, "%s(", this->fcnname?this->fcnname:"<fcn>");
		l = strlen(buf);
		for(i=0; this->params[i]; i++) {
			if (buf[l]) strncat(buf+l, ", ", len-l);
			l += strlen(buf+l);
			GetName(this->params[i], buf+l, len-l);
		}
		strncat(buf, ")", len);
	}
	return buf;
}


@+ Convenience Expression Notation
For more readable tests.. these allow you to specify expressions using nested function calls with variable-number of arguments. 

@c
#define END LL_CONSTANT(0)
#define VARARGS(p,tpe) (tpe*) varargs((void**) params,p,ap)
#define Column(op,colno) op->expr[colno] 

static void** varargs(void** params, void* cur, va_list ap) {
	int i = 0;
  	do {
		params[i++] = cur;
		cur = va_arg(ap,void*);
	} while(cur);
	params[i] = NULL;
	return params;
}

Expr *Function(str fcn, Expr* param, ...) {
	Expr *params[NPARAMS+1];
	va_list ap;
	va_start(ap, param);
	return FcnExpr(fcn, VARARGS(param,Expr*)); 
}

Expr *Constant(str buf) {
	char typename[1024];
	dbl val;
	uidx pos;
	Type *tpe;

	for(pos=0; buf[pos] != '('; pos++) typename[pos] = buf[pos];
	typename[pos] = 0;
	if ((tpe=GetType(typename)) == NULL) return NULL;
	tpe->value(buf+pos+1, (void*) &val, sizeof(dbl));
	return SetName(ConstExpr((void*) &val, GetType(typename)), buf);
}

Expr *ScalarConst(char* tpe, slng val) {
	char buf[128];
	sprintf(buf, "%s(%lld)", tpe, val);
	return Constant(buf);
}

Expr *Dimension(char* tpe, uidx domain) {
	return Function("indextile", 
				ConstExpr(&domain,GetType("uidx")),
				ConstExpr(&domain,GetType("uidx")),
				ScalarConst(tpe, domain), END);
}

@- Printing
@h
#define VectorValue(vec,pos)		((void*) (((str) (vec)->first) + (vec)->width * (pos)))
#define GetValue(sel,col,pos)		VectorValue(col->vec, (sel)?*(uidx*)VectorValue(sel,pos):(pos))
@c
str GetNameQuoted(Expr *this, str buf, size_t len) {
	buf[0] = 0;
	if (len > 2) {
		buf[0] = '"'; buf[1] = 0; 
		GetName(this, buf+1, len-2);
		strncat(buf, "\"", len);
	}
	return buf;
}

str StringValue(Vector* sel, Expr* this, uidx pos, str buf, size_t len) {
	this->tpe->string(GetValue(sel,this,pos), buf, len);
	return buf;
}


@* Relational Operators
@T
We implement a Volcano-like relational iterator model, where 
\begin{itemize}
\item {\bf init} is called Alloc() and coincides with all node allocation and initialization
\item {\bf next} is called Next() and returns the number of tuples found (can be more than one here).
\item {\bf close} is called Free() as it directly destroys the node and performs all garbage collection.
\end{itemize}

For now, we have implemented:
\begin{itemize}
\item scan: scan a table, in this case a set of [void,T] bats.
\item select: reduce the number of selected tuples with a boolean expression (i.e. scan-select)  
\item project: enrich the tuple stream with additional expressions, defined on its columns.
\item aggr: reduce the tuple stream with aggregation. The aggregate returns columns for all groupby 
expressions and all aggregate expressions. This is a blocking operation.
\end{itemize}

The {\bf memchunk} field indicates the size of the result vectors of the expressions returned
by this operator.

The {\bf sel} field is the selection vector of this node. If it is NULL, all elements in the
result vector count as result, otherwise only those elements indexed by the selection vector.
@h
typedef struct Operator_ {
	int evalid; /* magic id for executing shared branches in graphs only once */
	size_t memchunk; /* size of vectors in this pipeline */
	Vector *sel; /* output selection list. May be NULL if all in vector selected */
	struct Operator_ *son, *daughter; /* children. daughter not used yet (included for join)  */
	Expr *expr[NPARAMS+1]; /* roots of expression graph for this operator. terminated by NULL */
	size_t (*next)(struct Operator_*); /* fill the vectors with more tuples */
	void (*free)(struct Operator_*); /* cleanup */ 
	void* properties; /* table-wide properties */

	/* Aggr state */
	Expr *direct_group; /* in case of direct grouping, this points to the count aggregate (otherwise NULL) */
	Expr *groupid, *hashnum; /* expression that computes the global hash number */ 
	Expr *groupby[NPARAMS]; /* groupby expressions */
	int ngroupbys; 
	Vector *mask, *link, *miss, *match; 

	/* Scan state 
         * TODO: - put all info in a separate record
         *       - add async IO handle info here (array with handle for each BAT)  
         *       - administer positions for each column individually and use a global iochunk bytesize iso tuplecount 
         */
	BAT *bats[NPARAMS+1]; /* temporary bats for each column */ 
	int fds[NPARAMS+1]; /* open files in BUN heap of underlying (BIG) BATs*/ 
	uidx pos, end; /* current and last BUN in loaded chunk (half of temporary bat)  */
	size_t prefetched; /* number of tuples under prefetch in other half of temporary bat */
	size_t ntuples; /* number of tuples still not read or prefetched */
	size_t iochunk; /* chunksize in tuples to use on each BAT for this scan */
} Operator;

@+ Array
The array operator incrementally generates a table that represents a multidimensional array. Its parameters are a 
series of dimensions of a certain size, such that the resulting table contains all one row for each cell in the 
array, and its columns contain the respective array indices (starting at 0) using minor-first dimension ordering.
@c
size_t ArrayNext(Operator *this) {
	uidx n = MIN(this->end-this->pos, this->memchunk); 
	int i;

	this->evalid++;
	for(i=0; this->expr[i]; i++) {
		ExprEval(this->expr[i], n, NULL, this->evalid); 
	}
	this->pos += n;
	return n; 
}


void ArrayFree(Operator *this) {
	int i;
	for(i=0; this->expr[i]; i++) { 
		this->expr[i]->vec->ptr = this->expr[i]->vec->base;
		ExprFree(this->expr[i]);
	}
	FREE(this);
}


Operator *ArrayAlloc(size_t memchunk, Expr** dimension)  {
	Operator *this = (Operator*) ZALLOC(sizeof(Operator));
	Expr *pos = ConstExpr(NULL,GetType("uidx"));
	int i;

	this->end = 1;
	for(i=0; dimension[i]; i++) {
		ResolveFcn(dimension[i], "map", memchunk);
		this->end *=  *(uidx*) dimension[i]->params[0]->vec->first; /* 'hidden' mult */
		this->expr[i] = dimension[i];
		dimension[i]->refcnt++;

		/* let the value in the first params of indextile point to this->pos and this->end */
		dimension[i]->params[0]->vec->first = &this->pos;
		dimension[i]->params[1]->vec->first = &this->end;
	}
	this->memchunk = memchunk;
	this->next = ArrayNext;
	this->free = ArrayFree;
	return this;
}


@+ Scan
There is here already some attempt at doing asynchronous I/O: allocates a buffer twice iochunk size, and 
prefetches a ready on one half while the other half is being processed.
@c
size_t ScanRead(Operator *this) {
	int i;
	size_t pos=0, prefetched = this->prefetched;

	this->prefetched = MIN(this->iochunk, this->ntuples);
	this->ntuples -= this->prefetched;
	if (this->prefetched > 0) {
		if (this->pos == 0) {
			pos = this->iochunk; /* read into second half iso first */
		} 
		for(i=0; this->bats[i]; i++) {
			/* read directly from the BAT heap */ 
			BUN dst = BUNfirst(this->bats[i]) + BUNsize(this->bats[i])*pos;

			/* TODO: convert to asynchronous IO. block current ScanRead until previous invocation for that column has finished */ 
			size_t seenbytes, chunk, totbytes = this->ntuples * BUNsize(this->bats[i]);
			for(seenbytes=0; seenbytes<totbytes; seenbytes+=chunk)
				chunk = read(this->fds[i], dst+seenbytes, totbytes-seenbytes);
		}	 
	}
	return prefetched;
}
size_t ScanNext(Operator *this) {
	size_t i;
	size_t cnt = MIN(this->memchunk, this->end - this->pos);

	for(i=0; this->bats[i]; i++) {
		/* this is the place for enum decompression and differential list merging (into a separate vector, then) */
		this->expr[i]->vec->first = (void*) (BUNfirst(this->bats[i]) + this->pos*BUNsize(this->bats[i]));
		this->expr[i]->vec->size = cnt;

		/* memory/cache prefetch next vector */
		if (this->memchunk <= (this->end - this->pos))
			this->expr[i]->prefetch = ((char*) this->expr[i]->vec->first);
	}
	if (this->pos + cnt < this->end) {
		this->pos += cnt;
	} else {
		size_t prefetched = ScanRead(this);
		if (this->pos >= this->iochunk) {
			this->pos = 0;
		}
		this->end = this->pos+prefetched;
	} 
	this->evalid++;
	return cnt;
}

void ScanFree(Operator *this) {
	int i;
	for(i=0; this->bats[i]; i++) {
		if (this->fds[i] >= 0) close(this->fds[i]);
		BBPunfix(this->bats[i]->batCacheid);
		ExprFree(this->expr[i]);
	}
	FREE(this);
}


Operator *ScanAlloc(size_t iochunk, size_t memchunk, str *batnames)  {
	Operator *this = (Operator*) ZALLOC(sizeof(Operator));
	int i;
	size_t batsize = 2*(this->iochunk=iochunk);
	BAT *b;
	for(i=0; batnames[i]; i++) {
		char *tpe;
		/* BAT reading only implemented for fixed-size-tail void-head BATs!! */ 
		bat bid = BBPindex(batnames[i]);
		BBPfix(bid);
		b = (BAT*) BBPgetdesc(bid);
		this->expr[i] = ConstExpr(NULL, GetBatType(b, "ScanAlloc"));
		this->expr[i]->vec->size = memchunk;
		this->expr[i]->vec->width = BUNsize(b);
		this->expr[i]->refcnt++;
		SetName(this->expr[i], batnames[i]);
		if (batsize >= BATcount(b)) {
			this->end = batsize = BATcount(b);
			this->bats[i] = BATdescriptor(bid); /* use BAT itself */
			this->fds[i] = -1;
		} else {
			this->fds[i] = GDKfdlocate(b->batBuns->filename, "rb", NULL);
			lseek(this->fds[i], b->batBuns->offset + BUNsize(b) * ((size_t) b->batHole), SEEK_SET);
			this->bats[i] = BATnew(TYPE_void, b->ttype, batsize);
			this->ntuples = BATcount(b);
			this->pos = this->iochunk; /* trigger load into first half */
			this->bats[i]->batBuns->free = BUNsize(b)*batsize; /* make buns visible for debugging purposes */
			BATsetcount(this->bats[i], batsize);
			BBPunfix(bid);
		}
	} 
	this->memchunk = memchunk;
	this->next = ScanNext;
	this->free = ScanFree;
	ScanRead(this);
	this->pos = 0;
	this->evalid = 1;
	return this;
}


@+ Aggregate
Now only direct hashing on small-width types is implemented. There will also be support
for hashed grouping and sorted grouping. 
@c
size_t AggrNext(Operator *this) {
	size_t m, i, n = this->memchunk;
	if (this->evalid++ != 0) return 0;
	do {
		m = this->son->next(this->son);
		ExprEval(this->hashnum, m, this->son->sel, this->son->evalid); 
		for(i=this->ngroupbys; this->expr[i]; i++)
			ExprEval(this->expr[i], m, this->son->sel, this->son->evalid); 
	} while(m > 0);
	if (this->direct_group) 
		n = ExprEval(this->direct_group, this->memchunk, NULL, this->son->evalid); 
	this->evalid = this->son->evalid;
	return n; 
}

void AggrFree(Operator *this) {
	int i;
	if (this->direct_group) ExprFree(this->direct_group);
	if (this->mask) VectorFree(this->mask);
	if (this->link) VectorFree(this->link);
	if (this->miss) VectorFree(this->miss);
	if (this->match) VectorFree(this->match);
	if (this->sel) VectorFree(this->sel);

	for(i=0; i<this->ngroupbys; i++) 
		ExprFree(this->groupby[i]);
	for(i=0; this->expr[i]; i++)
		ExprFree(this->expr[i]);
	ExprFree(this->groupid);
	ExprFree(this->hashnum);
	this->son->free(this->son);
	FREE(this);
}

Operator *AggrAlloc(Operator *child, int include_groupbys, int ngroupbys, Expr **params) {
	Operator *this = (Operator*) ZALLOC(sizeof(Operator));
	int bits[NPARAMS], i, j, s;
	slng maxgroups = 0, maxbits=0, isdense=1;

	this->groupid = ConstExpr(NULL, GetType("uidx"));
	this->groupid->refcnt++;
	this->ngroupbys = include_groupbys?ngroupbys:0;

	/* determine grouping method; try to use direct grouping if possible */
	for(i=0; i<ngroupbys; i++) {
		uidx d = 0, m = 0, b;
		ResolveFcn(params[i], "map", child->memchunk);
 		b = (1<<(params[i]->tpe->width*8)) - 1; 
		if (params[i]->tpe->cardinal) {
			Vector *maxprop = GetProperty(params[0]->properties, "max");
			Vector *denseprop = GetProperty(params[0]->properties, "dense");
			Vector *upperprop = maxprop?maxprop:GetProperty(params[0]->properties, "upper");

			if (maxprop && i == 0) maxgroups = *(slng*) maxprop->first; /* max value as cardinal */
			if (upperprop) m = *(slng*) upperprop->first; /* max value as cardinal */
			if (denseprop && *(slng*) denseprop->first) 
				d = (i+1 == ngroupbys) || (maxprop && b == m+1); /* check full usage of the bit space */
			if (upperprop) b = m;
		}
		for(m=0; b; m++) b>>=1;
		maxbits += bits[i] = m; 
		isdense &= d;
		this->groupby[i] = params[i];
		this->groupby[i]->refcnt++;
	}

	if (maxbits <= isdense?17:24) { /* direct grouping on less than 4M (sparse) or 128K (dense) */
		if (ngroupbys == 1 && params[0]->tpe == GetType("uidx")) {
			this->hashnum = params[0];
		} else {
			/* create hashnum expression that generates hashnums, that in this case are direct dense group ids */
			j = ngroupbys-1;
			this->hashnum = Function("uidx", params[j], END);
			for(s = bits[j]; --j >= 0; s += bits[j]) {
				this->hashnum = Function("directgrp", 
							this->hashnum, 
							params[j], 
							ConstExpr((void*) &s, GetType("uint")), END);
			}
			ResolveFcn(this->hashnum, "map", child->memchunk);
			maxgroups = 0;
		}
		/* determine max # groups for allocating the result array */
		if (maxgroups == 0) maxgroups = 1<<maxbits; 
		this->memchunk = maxgroups; 

		this->hashnum->refcnt ++; 
		*this->groupid->vec = *this->hashnum->vec; /* in direct grouping, the groupid *is* the hash number */
		this->groupid->vec->ptr = NULL; /* makes groupid a non-singular expression */
		this->groupid->vec->refcnt = 1;


		/* initialize groupby result column as a cast over generated values */
		for(j=this->ngroupbys, s=0; --j >= 0; s += bits[j]) {
			Expr *e = (Expr*) ZALLOC(sizeof(Expr));
			void *ptr;
			e->tpe = GetType("uidx");
			e->vec = VectorAlloc(e->tpe, this->memchunk);
 			ptr = e->vec->ptr;
			e->vec->ptr = NULL; /* makes e a non-singular expression */
			e->vec->refcnt++;
			this->expr[j] = Function(params[j]->tpe->name, e, END);
			this->expr[j]->refcnt++;
			if (params[j]->name) SetName(this->expr[j], params[j]->name);
			ResolveFcn(this->expr[j], "map", this->memchunk);
			for(i=0; i<this->memchunk; i++)  /* generate base values */
				((uidx*) e->vec->first)[i] = (i & (((1<<bits[j])-1) << s)) >> s; 
			ExprEval(this->expr[j], this->memchunk, NULL, 1); /* compute casts from base values */

			/* free the now irrelevant base value expression */
			e->vec->ptr = ptr;
			this->expr[j]->params[0] = 0;
			ExprFree(e);
		}

		if (!isdense) {
			/* direct groupbys needs a count to filter out non-occuring values afterwards */
			for(i=ngroupbys; params[i]; i++) {
				if (strcmp((str) params[i]->fcnname, "count") == 0) {
					this->direct_group = params[i];
				}
			}
			if (this->direct_group == NULL) { /* introduce one if not found */
				this->expr[i] = this->direct_group = Function("count", END);
				this->expr[i]->refcnt++;
				this->expr[++i] = NULL;
			}
			this->direct_group->refcnt++;
		}
	} else {
		FATAL("AggrAlloc: hashed groupby nyi\n", i); 
	}
	/* resolve all aggregate expressions */
	for(i=ngroupbys,j=this->ngroupbys; params[i]; i++, j++) {
		this->expr[j] = params[i];
		this->expr[j]->refcnt++;
		for(s=0; params[i]->params[s]; s++) {
			/* aggregate function parameter vectors have child size! */
			ResolveFcn(params[i]->params[s], "map", child->memchunk);
		}
		/* append groupid expression as extra parameter */
		params[i]->params[s++] = this->groupid;
		this->groupid->refcnt++;

		/* aggregate function has aggr size! */
#if defined(ia64) 
		/* 4cursor aggregates need 4 times space */
		ResolveFcn(params[i], "aggr4", this->memchunk*4);

		/* add memchunk as additional (hidden) parameter so multi-cursor variants know the direct hashing size */
		params[i]->params[s] = ConstExpr(&maxgroups, GetType("uidx"));
		params[i]->params[s]->refcnt++;
#else
		ResolveFcn(params[i], "aggr", this->memchunk);
#endif
	}
	/* restore 'normal' size of result */
	this->memchunk = maxgroups; 

	if (this->direct_group) {
		/* the direct group aggregate introduces a select based on the count */
		this->direct_group = Function(">", this->direct_group, Constant("uidx(0)"), END);
		this->direct_group->refcnt++;
		ResolveFcn(this->direct_group, "select", this->memchunk);
		this->sel = this->direct_group->vec;
		this->sel->refcnt++;
	}
	this->son = child;
	this->next = AggrNext;
	this->free = AggrFree;
	return this;
}

@T
Below is a description of vectorized hash grouping.
\begin{verbatim}

#define sel(b,x,y,z) if (b) x[y++] = z
#define sel(b,x,y,z) { bool _b = b; x[y] = z; y += _b};

vectorized multi-attribute groupby
==================================

hash(S,V,O) 		= O[i] = HASH(V[i])
rehash(S,V,O) 		= O[i] ^= HASH(V[i])
hashmask(S,H,M,m,O) 	= { int j = M[H[i]&m]; if (j) O[ret++] = (j<<32) | i; } 
hashnext(S,C,N,O) 	= { int k = C[i]&UINT_MAX, j = N[k]; if (j) O[ret++] = (j<<32) | k; } 
hashselect(S,I,V,G,O)	= if (eq(V[I[i]&UINT_MAX], G[i>>32]) O[ret++] = I[i]  
hashput(S,I,O) 		= O[I[i]&UINT_MAX] = (I[i]>>32));

Alloc:
=====
this->hashnum = VectorAlloc(sizeof(uint), this->son->memchunk);
this->groupid = VectorAlloc(sizeof(uint), this->son->memchunk);
this->mask    = VectorAlloc(sizeof(uint), 65536);
this->next    = VectorAlloc(sizeof(uint), 65536);
this->miss    = VectorAlloc(sizeof(uint), this->son->memchunk);
this->match   = VectorAlloc(sizeof(uidx), this->son->memchunk);

for(i=0;i<this->ngroupbys; i++) { 
	this->groupby[i] = this->expr[i];
	this->expr[i] = VectorAlloc(this->expr[i]->width, 65536);
}
for(i=ngroupbys;expr[i]; i++) {
	# add this->hashnum as last param
	for(j=0;this->expr[i]->params[j]; j++) {
		this->expr[j]->params = this->groupid;
		this->expr[j+1]->params = NULL;
	}
}

Next:
====

while((m=this->son->next()) > 0) {
   	Vector selbuf, *sel=this->son->sel;
	size_t n = sel?sel->size:this->son->memchunk, miss_cur=0, miss_max=0;
	
	for(i=0;i <ngroupbys; i++)
		ExprEval(this->groupby[i], m, this->son->sel, this->evalid);

	# compute hash numbers for all tuples
	map_hash64k_any_col1_uint(n, sel->first, groupby[0]->first, this->hashnum->first);
	for(i=1;i<ngroupbys; i++)
		map_rehash64k_uint_col1_any_col2_uint(n, sel->first, this->hashnum->first, groupby[i]->first, this->hashnum->first);

	do { 
		size_t l = 0;
                n = sel?sel->size:this->son->memchunk;
   
		for(i=0; i<this->groupid->size; i++) ((int*) this->groupid->first)[i] = max_uint;	
		this->match->size = hashmask(n, sel->first, this->hashnum->first, this->mask->first, this->match->first);
		while(this->match->size > 0) { # add collisions in bulk
			for(i=0; i<ngroupbys; i++) # reduce matches column-wise
				this->match->size = hashselect_any(this->match->size, this->match->first, groupby[i]->first, expr[i]->first, this->match->first);
			hashput(this->match->size, this->match->first, this->groupid->first); # put group-ids in this->groupid
			this->match->size = hashnext(this->match->size, NULL, this->match->first, this->next->first, this->match->first);
		}
		this->miss->size = sel_uint_col_uint_val(this->match->size, this->groupid->first, &max_int, this->miss->first); 

       		# handle misses on a tuple-at-a-time basis; in aggregation, it should be infrequent
		if (this->miss->size > 0) {
			/* insert last miss tuple in hash structure */ 
			uidx x = ((uidx*) this->miss->first)[--this->miss->size];
			uidx y = ((uidx*) this->mask->first)[((uidx*) this->hashnum->first)[x]];
			((uidx*) this->mask->first)[((uidx*) this->hashnum->first)[x]] = x;
			VectorExtend(this->next, GetType("uidx"), 1);
			((uidx*) this->next->first)[x] = y; 
			for(i=0; i<ngroupbys; i++) { 
				size_t width = expr[i]->width;
				VectorExtend(expr[i], expr[i]->tpe, 1);
				memcpy(expr[i]->first + width*x, groupby[i]->first + width*x, width);
			}
			# put next missing tuple as sole selection
			sel = &sel_BUF;
			sel_BUF.size = 1; 
			sel_BUF.ptr = this->miss->ptr + miss_cur++;
			if (miss_max == 0) miss_max = this->miss->size; 
		}
	} while(miss_cur < miss_max);
    
	# result group ids are in this->hashnum 
	for(i=ngroupbys; this->expr[i]; i++)
		ExprEval(this->expr[i], m, this->son->sel, this->evalid);
}
\end{verbatim}

@+ Select
Very simple: scan-select. Notice that thanks to builtin support for indirect adressing it only entails
filling a new selection vectors with indices. 
@c
size_t SelectNext(Operator *this) {
	size_t m = 0;
	do {
		m = this->son->next(this->son);
		this->evalid = this->son->evalid;
		if (m == 0) break;
		m = ExprEval(this->expr[0], m, this->son->sel, this->evalid); 
	} while (m == 0);
	return m;
}

void SelectFree(Operator *this) {
	ExprFree(this->expr[0]);
	if (this->sel) VectorFree(this->sel);
	this->son->free(this->son);
	FREE(this);
}

Operator *SelectAlloc(Operator *child, Expr *condition) {
	Operator *this = (Operator*) ZALLOC(sizeof(Operator));

	this->memchunk = child->memchunk;
	if (condition->fcnname == NULL) {
		condition = Function("", condition, END);
	}
	this->expr[0] = condition;
	this->expr[0]->refcnt++;
	ResolveFcn(this->expr[0], "select", this->memchunk);
	this->sel = condition->vec;
	this->sel->refcnt++;
	this->son = child;
	this->next = SelectNext;
	this->free = SelectFree;
	return this;
}

@+ Chunk
Chunk re-packages the tuple stream in a different granularity. It also eliminate sparseness if the child tuple stream has a selection vector. 
@c
size_t ChunkNext(Operator *this) {
	size_t i, batch;
	while(this->pos - this->end < this->memchunk) {
		if (this->pos >= this->memchunk) {
			/* copy to start of vector */
			this->end = this->end - this->pos;
			this->pos = 0;
			for(i=0; this->expr[i]; i++) 
				memcpy(this->expr[i]->vec->base, this->expr[i]->vec->first, this->expr[i]->tpe->width * this->end);
		}
		/* set the vector starts to their end (in order to append to them) */ 
		for(i=0; this->expr[i]; i++) 
			this->expr[i]->vec->first = (void*) (((char*) this->expr[i]->vec->base) + this->expr[i]->tpe->width * this->end);

		/* append new values */
		batch = this->son->next(this->son);
		if (batch == 0) break;
		for(i=0; this->expr[i]; i++) 
			ExprEval(this->expr[i], batch, this->son->sel, this->son->evalid); 
		this->end += batch;
	} 
	/* set the vector starts to their start position (to return a result) */ 
	for(i=0; this->expr[i]; i++) 
		this->expr[i]->vec->first = (void*) (((char*) this->expr[i]->vec->base) + this->expr[i]->tpe->width * this->pos);

	batch = MIN(this->memchunk, this->end - this->pos);
	this->pos += batch;
	if (batch) this->evalid++;
	return batch;
}

void ChunkFree(Operator *this) {
	int i;
	for(i=0; this->expr[i]; i++) 
		ExprFree(this->expr[i]);
	this->son->free(this->son);
	FREE(this);
}

Operator *ChunkAlloc(Operator *child, size_t memchunk, Expr** expr) {
	Operator *this = (Operator*) ZALLOC(sizeof(Operator));
	int i;
	for(i=0; expr[i]; i++) {
		ResolveFcn(expr[i], "map", child->memchunk);
		if (child->sel) {
			/* in case of a selection, we use fetch to create a compact result */
			Expr *sel = ConstExpr(NULL, GetType("uidx"));
			Expr *col = (Expr*) ZALLOC(sizeof(Expr));
			*sel->vec = *child->sel;
			sel->vec->ptr = NULL; /* will make it a non-singular expression */
			col->tpe = expr[i]->tpe;
			col->vec = expr[i]->vec;
			col->vec->refcnt++;
			this->expr[i] = Function("fetch", sel, col, END);
			ResolveFcn(this->expr[i], "map", child->memchunk);
		} else {
			this->expr[i] = expr[i];
		}
		VectorRealloc(this->expr[i]->vec, this->expr[i]->tpe, child->memchunk + memchunk); /* repackaging needs some extra space */
		this->expr[i]->refcnt++;
	}
	this->memchunk = memchunk;
	this->son = child;
	this->next = ChunkNext;
	this->free = ChunkFree;
	return this;
}

@+ Project
Very simple: just compute a bunch of expressions on the same vector size as your child, and using its same selection vector.
@c
size_t ProjectNext(Operator *this) {
	int i;
	size_t n = this->son->next(this->son);

	this->evalid = this->son->evalid;
	for(i=0; this->expr[i]; i++)
		ExprEval(this->expr[i], n, this->sel, this->evalid); 
	return n;
}

void ProjectFree(Operator *this) {
	int i;
	for(i=0; this->expr[i]; i++)
		ExprFree(this->expr[i]);
	this->son->free(this->son);
	FREE(this);
}

Operator *ProjectAlloc(Operator *child, Expr **params) {
	Operator *this = (Operator*) ZALLOC(sizeof(Operator));
	int i;
	size_t n;

	this->memchunk = child->memchunk;
	for(i=0; params[i]; i++) {
		this->expr[i] = params[i];
		this->expr[i]->refcnt++;
		ResolveFcn(this->expr[i], "map", this->memchunk);
	} 
	this->sel = child->sel;
	this->son = child;
	this->next = ProjectNext;
	this->free = ProjectFree;
	return this;
}

@+ FetchJoin
Use an uidx to get the N-th row in a table (i.e. implicit join on ROWID without real join keys).
It uses the efficient "map_fetch_uidx_col_TPE_col" primitives to do this.
This should exploit an array-based tuple layout for very fast performance. 

In the future, we must integrate knowledge about radix-clustered "uidx" sequences, and load
only a chunks of the column instead of an entire BAT (plus some juggling with the base pointers)

@= FetchJoinNext
size_t FetchJoinNext@2(Operator *this) {
	size_t n = this->son->next(this->son);
	uidx off;
	char *ptr;
	int i;

	this->evalid = this->son->evalid;
	if (n > 0) 
	for(i=0; this->expr[i]; i++) {
		ExprEval(this->expr[i], n, this->sel, this->evalid); 

		/* prefetch next vector assuming sequential access 
		off = 1 + ((@1*) this->expr[i]->params[0]->vec->first)[n-1]; /* get last index * /
		ptr = ((char*) this->expr[i]->vec->first) + off*this->expr[i]->tpe->width;
		if (ptr + this->memchunk*this->expr[i]->tpe->width < BUNlast(this->bats[i])) 
			this->expr[i]->prefetch = ptr;
		*/
	}
	return n;
}
@c
@:FetchJoinNext(uidx)@
@:FetchJoinNext(dbl,_dbl)@

void FetchJoinFree(Operator *this) {
	int i;
	for(i=0; this->expr[i]; i++) 
		BBPunfix(this->bats[i]->batCacheid);
	ProjectFree(this);
}

Operator *FetchJoinAlloc(Operator *child, Expr* index, str* batnames)  {
	Operator *this = (Operator*) ZALLOC(sizeof(Operator));
	int i;
	for(i=0; batnames[i]; i++) {
		bat bid = BBPindex(batnames[i]);
		Expr *base;

		BBPfix(bid);
		this->bats[i] = BATdescriptor(bid);
		base = ConstExpr(NULL, GetBatType(this->bats[i], "FetchJoinAlloc"));
		base->vec->first = (void*) BUNfirst(this->bats[i]);
		this->expr[i] = Function("fetch", index, base, END);
		ResolveFcn(this->expr[i], "map", child->memchunk);
		this->expr[i]->refcnt++;
	}
	this->memchunk = child->memchunk;
	this->sel = child->sel;
	this->son = child;
	this->next = (index->tpe == GetType("dbl"))?FetchJoinNext_dbl:FetchJoinNext;
	this->free = FetchJoinFree;
	return this;
}


@- Convenience Operators
For more readable tests.. these allow you to specify query plans using nested function calls with variable-number of arguments. 
@c
#define Select(child, condition, end) SelectAlloc(child, condition)

Operator *Array(size_t memchunk, Expr *expr, ...) {
	Expr *params[2*NPARAMS+1];
	va_list ap;
	va_start(ap, expr);
	return ArrayAlloc(memchunk, VARARGS(expr,Expr*)); 
}

Operator *Scan(size_t iochunk, size_t memchunk, str batname, ...) {
	str params[NPARAMS+1];
	va_list ap;
	va_start(ap, batname);
	return ScanAlloc(iochunk, memchunk, VARARGS(batname,str)); 
}

Operator *Aggr(Operator *child, int include_groupbys, int ngroupbys, Expr *expr, ...) {
	Expr *params[2*NPARAMS+1];
	va_list ap;
	va_start(ap, expr);
	return AggrAlloc(child, include_groupbys, ngroupbys, VARARGS(expr,Expr*)); 
}

Operator *Chunk(Operator *child, size_t memchunk, Expr *expr, ...) {
	Expr *params[NPARAMS+1];
	va_list ap;
	va_start(ap, expr);
	return ChunkAlloc(child, memchunk, VARARGS(expr,Expr*)); 
}

Operator *Project(Operator *child, Expr *expr, ...) {
	Expr *params[NPARAMS+1];
	va_list ap;
	va_start(ap, expr);
	return ProjectAlloc(child, VARARGS(expr,Expr*)); 
}

Operator *FetchJoin(Operator* child, Expr *index, str batname, ...) {
	str params[NPARAMS+1];
	va_list ap;
	va_start(ap, batname);
	return FetchJoinAlloc(child, index, VARARGS(batname,str)); 
}

@* x100 Test Cases
@m
.MODULE x100;

.USE uchr, monettime;

.COMMAND vectorized_q1(
	str l_returnflag, str l_linestatus, str l_quantity, str l_extendedprice, str l_discount, str l_tax, str l_shipdate, 
	date *hi_date_ptr, int memchunk, lng iochunk) : str = vectorized_q1; "Return the result of TPC-H Q1 in XML. Last two parameters: memchunk is the number of tuples to pipeline, iochunk the number of tuples to read from disk at once in a scan."

.COMMAND vectorized_stddev(str bat_mu, str bat_s2, str bat_x, int memchunk, int m, int c, int d, int i) : BAT[void,dbl] = vectorized_stddev;
       "computes standard deviation over a number of images. returns an array bat of dimensionality (m,c,d)"  

@mil
l1 := bat(void,chr).insert(nil,'a').insert(nil,'b').insert(nil,'b').seqbase(0@0).access(BAT_READ).rename("l1");
l2 := bat(void,chr).insert(nil,'c').insert(nil,'c').insert(nil,'c').seqbase(0@0).access(BAT_READ).rename("l2");
l3 := bat(void,flt).insert(nil,1.0).insert(nil,2.0).insert(nil,3.0).seqbase(0@0).access(BAT_READ).rename("l3");
l4 := bat(void,flt).insert(nil,10.0).insert(nil,20.0).insert(nil,30.0).seqbase(0@0).access(BAT_READ).rename("l4");
l5 := bat(void,flt).insert(nil,0.0).insert(nil,0.2).insert(nil,0.4).seqbase(0@0).access(BAT_READ).rename("l5");
l6 := bat(void,flt).insert(nil,0.2).insert(nil,0.2).insert(nil,0.2).seqbase(0@0).access(BAT_READ).rename("l6");
l7 := bat(void,date).insert(nil,date(2003,1,1)).insert(nil,date(2003,1,2)).insert(nil,date(2003,1,3)).seqbase(0@0).access(BAT_READ).rename("l7");

# the ram query
m := 1320;
c := 8;
i := 14;
d := 100;
x := bat(void,dbl); j := 0; while((j:+=1) <= (m * i)) x.insert(nil,dbl(1)); x.seqbase(0@0).access(BAT_READ).rename("x");
mu := bat(void,dbl); j := 0; while((j:+=1) <= (i * c * d)) mu.insert(nil,dbl(1)); mu.seqbase(0@0).access(BAT_READ).rename("mu");
s2 := bat(void,dbl); j := 0; while((j:+=1) <= (i * c * d)) s2.insert(nil,dbl(1)); s2.seqbase(0@0).access(BAT_READ).rename("s2");

table(l1, l2, l3, l4, l5, l6, l7);

printf("\nvectorized_q1(\"l1\",\"l2\",\"l3\",\"l4\",\"l5\",\"l6\",\"l7\",date(2003,1,3),1,1000LL).printf;\n");
printf("\nvectorized_stddev(\"mu\",\"s2\",\"x\",1,m,c,d,i).print;\n");
@m

.END x100;

@+ Test1: TPC-H Query 1
@- SQL Query
@T
\begin{verbatim}
select 
	L.returnflag, 
	L.linestatus, 
	sum(L.quantity) as sum_qty, 
	sum(L.extendedprice) as sum_base_price, 
	sum(L.extendedprice*(1-L.discount)) as sum_disc_price, 
	sum(L.extendedprice*(1-L.discount)*(1+L.tax)) as sum_charge, 
	avg(L.quantity) as avg_qty, 
	avg(L.extendedprice) as avg_price, 
	avg(L.discount) as avg_disc, 
	count(*) as count_order 
from 
	lineitem L
where 
	L.shipdate <= date '1998-12-01' - interval '[DELTA]' day (3) 
group by 
	L.returnflag, 
	L.linestatus 
order by 
	L.returnflag, 
	L.linestatus; 
\end{verbatim}

@- X100 Algebra Query
@T
\begin{verbatim}
Project(Aggr(Select(Scan(%MEMCHUNK,%IOCHUNK, 
                        [l_returnflag=Column("L.returnflag"), l_linestatus=Column("L.linestatus"),
                         l_quantity=Column("L.quantity"), l_extendedprice=Column("L.extendedprice"),
                         l_discount=Column("L.discount"), l_tax=Column("L.tax", l_shipdate=Column("L.shipdate"])
              <=(l_shipdate,%DATE)),
         TRUE, 2, 
        [l1 = l_returnflag, l2 = l_linestatus
         sum_qty = sum(l_quantity),
         sum_base_price = sum(l_extendedprice),
         sum_disc_price = sum(discountprice = (1.0-l_discount)*l_extendedprice),
         sum_charge = sum((tax+1.0)*discountprice),
         sum_disc = sum(discount),
         count = count()])
    [l1, l2, sum_qty, sum_base_price, sum_disc_price, sum_charge,
     avg_qty = sum_qty/flt_count = flt(count), 
     avg_price = sum_base_price/flt_count, 
     avg_disc = sum_disc/flt_count, 
     count]
\end{verbatim}
@c
int vectorized_q1(
	str* result, str l_returnflag, str l_linestatus, str l_quantity, str l_extendedprice, str l_discount, str l_tax, str l_shipdate, 
	date *hi_date_ptr, int *memchunk, lng *iochunk)
{
	char colbuf[1024], valbuf[1024];
	slng n = 127;
	int j, i;

        Operator *op_scan = Scan((size_t) *iochunk, *memchunk, 
				l_returnflag, l_linestatus,  l_quantity, l_extendedprice, l_discount, l_tax, l_shipdate, END); 
	Type *lng_t = GetType("slng");
	Expr *shipdate      = Column(op_scan,6);

	Operator *op_sel = 
		Select(op_scan, Function("<=", shipdate, ScalarConst("uint",*hi_date_ptr), END), END);
#if 0
	Operator *op_chunk = 
		Chunk(op_sel, *memchunk *2, Column(op_scan,0), Column(op_scan,1), Column(op_scan,2), 
					       Column(op_scan,3), Column(op_scan,4), Column(op_scan,5), END); 
	Operator *op_select = op_chunk;
#else 
	Operator *op_chunk = op_scan;
	Operator *op_select = op_sel;
#endif
	Expr *returnflag    = SetProperty(Column(op_chunk,0), "upper", lng_t, VectorConst(lng_t, &n));
	Expr *linestatus    = SetProperty(Column(op_chunk,1), "upper", lng_t, VectorConst(lng_t, &n));
	Expr *quantity      = Column(op_chunk,2);
	Expr *extendedprice = Column(op_chunk,3);
	Expr *discount      = Column(op_chunk,4);
	Expr *tax           = Column(op_chunk,5);

	Expr *discountprice = 	
		Function("*", Function("-", Constant("flt(1.0)"), discount, END),
			      extendedprice, END);  
	Operator *op_aggr = 
		Aggr(op_select, TRUE, 2, returnflag, linestatus,
			SetName(Function("sum", quantity, END), "sum_qty"),  
			SetName(Function("sum", extendedprice, END), "sum_base_price"),  
			SetName(Function("sum", discountprice, END), "sum_disc_price"),  
			SetName(Function("sum", Function("*", Function("+", tax, Constant("flt(1.0"), END), 
					    	      	      discountprice, END), END), "sum_charge"),
			Function("sum", discount, END),
			Function("count", END), END);

	Expr *dbl_count = Function("dbl", Column(op_aggr,7), END); 
	Operator *op_result = 
		Project(op_aggr, 
			Column(op_aggr,0), 
			Column(op_aggr,1),
			Column(op_aggr,2),
			Column(op_aggr,3), 
			Column(op_aggr,4),
			Column(op_aggr,5),
			SetName(Function("/", Column(op_aggr,2), dbl_count, END), "avg_qty"),
			SetName(Function("/", Column(op_aggr,3), dbl_count, END), "avg_price"),
			SetName(Function("/", Column(op_aggr,6), dbl_count, END), "avg_disc"),
			Column(op_aggr,7), END);

	*result = (str) GDKmalloc(8192);
	snprintf(*result, 8192, "<query name=tpch_q1 hi_date=%d>\n", (int) *hi_date_ptr);
#ifdef PROFILE
		{ ulng profiling = x100_clock();
#endif
	while((n = op_result->next(op_result)) > 0) {
		for(i=0; i<n; i++) {
			Expr *col;
			strncat(*result, "    <row>\n", 8192);
			for(j=0; (col=Column(op_result,j)) != NULL; j++) {
				int len = strlen(*result);
				snprintf(*result+len, 8192-len, "        <attr name=% 16s   type=\"%s\">%s</attr>\n", 
					GetNameQuoted(col,colbuf,1024), col->tpe->name, StringValue(op_result->sel,col,i,valbuf,1024));
			}
			strncat(*result, "    </row>\n",8192);
		}
	}
#ifdef PROFILE
		profiling = x100_clock() - profiling;
		printf("total %lld\n", profiling );
		}
#endif
	strncat(*result, "<query>\n",8192);


	profileOperator(op_result, "");
	op_result->free(op_result);
	return GDK_SUCCEED;
}

@+ Test2: Image Standard Deviation Over Multiple Samples 
@- RAM Query
@T
\begin{verbatim}
[ sum([((X[s,i]-Mu[d,c,i])^"2.0LL")/S2[d,c,i] | d<D, s<M, c<C, i<I][d,s,c,_]) | d<D, s<M, c<C ][d,s,_]) | d<D, s<M ]
\end{verbatim}

@- X100 Algebra Query
@T
\begin{verbatim}
input:
	table samples(dbl X)
	table stddist(dbl mu, dbl s2)

example %d=100
	%i=14
	%c=8
	%m=1320

Aggr(FetchJoin(FetchJoin(Project(Array([I = Dimension(%i), M = Dimension(%m), C = Dimension(%c), D = Dimension(%d)]),
                                 [ IDX1 = +(I, *(%i, M)), 
                                   IDX2 = +(I, *(%i, +(C, *(%c, D)))), 
                                   IDX3 = +(M, *(%m, +(C, *(%c, D)))) ]),
                         IDX1, [X = samples.x]),
               IDX2, [MU = stddist.mu, S2 = stddist.s2])
     [IDX3], [sum(/(*(XMU = -(X,MU), XMU), S2))])
\end{verbatim}
@c
#if defined(ia64) 
#define IDXTPE "dbl"
#define IDXCAST(e) Function("uidx",e,END)
#else
#define IDXTPE "uidx"
#define IDXCAST(e) e
#endif
int vectorized_stddev(BAT **res, str mu, str s2, str x, int *memchunk, int *m, int *c, int *d, int *i) {
	Expr *M = Dimension(IDXTPE,*m), *C = Dimension(IDXTPE,*c), *I = Dimension(IDXTPE,*i), *D = Dimension(IDXTPE,*d); 
	Operator *ar = Array(*memchunk, I, C, M, D, END);
	slng j = 1, n = *m * *c * *d;
	Type *lng_t = GetType("slng"), *t = M->tpe;
	Expr *idx1 = Function("+", I, Function("*", ScalarConst(IDXTPE,*i), M, END), END);
	Expr *idx2 = Function("+", I, Function("*", ScalarConst(IDXTPE,*i), Function("+", C, Function("*", ScalarConst(IDXTPE,*c), D, END), END), END), END);
	Expr *idx3 = IDXCAST(Function("+", C, Function("*", ScalarConst(IDXTPE,*c), Function("+", M, Function("*", ScalarConst(IDXTPE,*m), D, END), END), END), END));
	Operator *compr = Project(ar, idx1, idx2, END);
	Operator *get_x = FetchJoin(compr, Column(compr,0), x, END);
	Operator *get_mu = FetchJoin(get_x, Column(compr,1), mu, s2, END);
	Expr *x_mu = Function("-", Column(get_x,0), Column(get_mu,0), END);
	Operator *aggr = Aggr(get_mu, FALSE, 1, SetProperty(SetProperty(idx3, "max", lng_t, VectorConst(lng_t, &n)), "dense", lng_t, VectorConst(lng_t, &j)),
				Function("sum", Function("/", Function("*", x_mu, x_mu, END),
							      Column(get_mu, 0), END), END), END);

	BAT *bn = *res = BATnew(TYPE_void, TYPE_dbl, n);
	dbl *cur = (dbl*) BUNfirst(bn);

#ifdef PROFILE
{ ulng profiling = x100_clock();
  ulng prf = 0;
#endif
	Mem2Cache(cur, 128*sizeof(dbl));
	while((n = aggr->next(aggr)) > 0) {
#ifdef PROFILE
		ulng t0 = x100_clock();
#endif
		for(j=0; j+256<n; j+=128) {
			int k;
			/* athlon should do cache2mem */
			Mem2Cache(cur + 128, 128*sizeof(dbl));
			for(k=j; k<j+128; k++) {
				*cur++ = *(dbl*) GetValue(aggr->sel, Column(aggr,0), k);
			}
		}
		for(; j<n; j++) {
			*cur++ = *(dbl*) GetValue(aggr->sel, Column(aggr,0), j);
		}
#ifdef PROFILE
		prf += (x100_clock() - t0);
#endif
	}
	bn->batBuns->free = ((BUN) cur) - BUNfirst(bn);
	BATsetcount(bn, bn->batBuns->free/BUNsize(bn));
	bn->tsorted = 0;
	BATseqbase(bn, 0);
#ifdef PROFILE
 profiling = x100_clock() - profiling;
 printf("total %lld %lld\n", profiling, prf );
}
#endif
	profileOperator(aggr, "");
	return GDK_SUCCEED;
}
