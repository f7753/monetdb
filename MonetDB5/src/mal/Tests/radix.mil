
#line 688 "/ufs/mk/monet_4-3/src/modules/plain/radix.mx"
module(radix);

var b := view_bbp_name.reverse;

# create data tables
if (not(b.exist("smaller_key")))
  uniform(1000000,1000000).reverse.mark(0@0).reverse.rename("smaller_key").persists(true).mmap(1);
if (not(b.exist("smaller_a1")))
  smaller_key.mirror.copy.rename("smaller_a1").persists(true).mmap(1);  
if (not(b.exist("smaller_aY")))
  smaller_key.copy.rename("smaller_aY").persists(true).mmap(1);  

if (not(b.exist("larger_key")))
  bat(void,int,2000000).insert(smaller_key).insert(smaller_key).seqbase(0@0).rename("larger_key").persists(true).mmap(1);
if (not(b.exist("larger_b1")))
  larger_key.mirror.copy.rename("larger_b1").persists(true).mmap(1);  
if (not(b.exist("larger_bZ")))
  larger_key.copy.rename("larger_bZ").persists(true).mmap(1);  

if (not(b.exist("smaller_all")))
   [integer]([int]([integer](smaller_key,1).reverse),16).reverse.rename("smaller_all").persists(true).mmap(1);
if (not(b.exist("larger_all")))
   [integer]([int]([integer](larger_key,1).reverse),16).reverse.rename("larger_all").persists(true).mmap(1);

# 0) SQL benchmark query
# ======================
#
# SELECT smaller.a1, smaller.aY, larger.b1, larger.bZ
# FROM   smaller, larger
# where  smaller.key = larger.key

# 1) cache-conscious-monet-join-strategy, optimized for a 256KB L2 cache of 32 bytes line width
# =============================================================================================
#
# first radix cluster both key columns on H bits (maybe different number of passes and bit settings)
#
# We have a 256KB cache, but want to fit comfortable in 128KB. Given a 8-byte relation width an 8-byte hash table, the 
# chunk size is 128KB/16 = 8192, and since we have a 1M inner relation this leads to 7 bits clustering, wich we do in 2 
# passes (4+3) to fit the 64-entry TLB
#
cluster_larger := radix_cluster(larger_key, 4, 3);
#250
cluster_smaller := radix_cluster(smaller_key, 4, 3);
#626

# phash, followed by radix-sort
#
# As we have 2M tuples with max value 2000000, we need to cluster on 21 bits (7+7+7) in order to achieve radix-sort.
#
res_join := phash_join(cluster_larger, cluster_smaller.reverse, 7, 2, false).reverse.radix_cluster(7,7,7).reverse;
#2.7
res_larger_sorted := res_join.mark(0@0).reverse; 

# no longer needed
cluster_larger := 0;
cluster_smaller := 0;

# positional-join projected columns from larger table into result
res_b1 := join(res_larger_sorted, larger_b1);
#570
res_bZ := join(res_larger_sorted, larger_bZ);
#570

# no longer needed
res_larger_sorted := 0;

# Given a 128KB of 'comfortable' L2, and 4-byte tuples in smaller_aX, we want chunk sizes of 32768. As we have a 
# cardinality of 2M, we create 64 chunks by partial radix-cluster on 6 bits. The maximum value is 1M, hence there 
# are 20 significant bits, so we ignore the lower 20-6=14 bits.
#
res_smaller_clustered := res_join.reverse.mark(0@0).reverse.radix_cluster(-14,6); 
#589
#344

# no longer needed
res_join := 0;

# positional-join and decluster projected columns from smaller table into result
#
# The window size of the matching window is again the comfortable 128KB, with 4 byte wide tuples its tuple size is 
# 32768. As we have 64 clusters, we can use a multiplier of 512. With 64 cluster, TLB trouble is avoided as well. 
#
borders_smaller := res_smaller_clustered.radix_count(14, 6);
res_a1 := join(res_smaller_clustered, smaller_a1).radix_decluster(borders_smaller, 512);
res_aY := join(res_smaller_clustered, smaller_aY).radix_decluster(borders_smaller, 512);

# no longer needed
res_smaller_clustered := 0;

print(res_b1.slice(1000000,1000100).col_name("b1"), res_bZ.col_name("bZ"), res_a1.col_name("a1"), res_aY.col_name("aY"));
print(res_b1.count);

# no longer needed
res_a1 := 0;
res_aY := 0;
res_b1 := 0;
res_bZ := 0;

# 2) cache-conscious-relational-join-strategy 
# ===========================================

smaller_view := [integer](smaller_all.reverse, 2).reverse;
larger_view := [integer](larger_all.reverse, 2).reverse;

# the inner relation will be 12+8 = 24 bytes wide, we have 128KB of cache hence can have chunk sizes of 5000.
# given an inner relation size of 1M tuples, this leads to  256 clusters of 4096, hence 8 bytes.
cluster_smaller := radix_cluster(smaller_view, 4, 4);
cluster_larger := radix_cluster(larger_view, 4, 4);
res_join := phash_join(cluster_larger, cluster_smaller.reverse, 8, 2, false);

# no longer needed
cluster_smaller := 0;
cluster_larger := 0;
 
print(res_join.slice(1000000,1000100));
res_join.count.print;

# alternatively, we try without clustering
res_join := phash_join(larger_view, smaller_view.reverse, 0, 2, false);

print(res_join.slice(1000000,1000100));
res_join.count.print;

larger_view := [integer](larger_all.reverse, 2).reverse.copy;
smaller_view := [integer](smaller_all.reverse, 2).reverse;
res_join := phash_join(larger_view, smaller_view.reverse, 0, 2, false);

print(res_join.slice(1000000,1000100));
res_join.count.print;


#quick test
#==========

res_join := uniform(2000000,999999).[oid];
res_smaller_clustered := res_join.reverse.mark(0@0).reverse.radix_cluster(-12,8);
borders_smaller := res_smaller_clustered.radix_count(12, 8);

res_smaller := res_join.reverse.mark(0@0).reverse;
t := time; res_a1 := join(res_smaller, smaller_a1); print(time - t);

cl_old := res_smaller_clustered.reverse.mark(0@0).reverse; 
cl_new := res_smaller_clustered.mark(0@0).reverse; 

t := time; cl_tmp := join(cl_old,  smaller_a1); print(time - t);
t := time; res_a1 := radix_decluster2(cl_new, cl_old, borders_smaller, 32);print(time - t);

t := time; b := join(res_smaller_clustered,  smaller_a1); print(time - t);
t := time; res_a1 := radix_decluster(b, borders_smaller, 16);print(time - t);


